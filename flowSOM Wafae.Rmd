---
title: "Protocol for FlowSOM implementation"
output: html_notebook
---

NOTE: *R version 4.5.0*. Also make sure [Rtools45](https://cran.r-project.org/bin/windows/Rtools/rtools45/rtools.html) is downloaded.

## STEP 0.1: DOWNLOADING PACKAGES (SKIP IF ALREADY DOWNLOADED)

Navigate towards 'Tools \> Install Packages' and download the following packages:

1.  *ggplot2*
2.  *ggpubr*
3.  *pheatmap*
4.  *tidyr*
5.  *devtools*

Then go on to download *BioConductor* in the following way in order to download *FlowCore* correctly.

```{r}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

#The following initializes usage of Bioc devel
BiocManager::install(version='devel')
BiocManager::install("flowCore", force = TRUE)
BiocManager::install("flowAI")
BiocManager::install("ggcyto")
BiocManager::install("openCyto")

#use devtools to download packages from github
devtools::install_github("saeyslab/PeacoQC", force=TRUE)
devtools::install_github("saeyslab/FlowSOM")
```

# PREPROCESSING

## STEP 1: PREPROCESSING: STANDARDIZATION + TRANSFORMATION STEPS.

The code below selects the patient files based on their patient_id from the .LMD files. The .LMD files contain two versions of .fcs files: Version 3.0 or Version 2.0. In this pipeline version 3.0 is selected as these contain the spillover matrices for compensation.

*NOTE: preprocessing full dataset can be heavy for CPU, better to preprocess in batches. + good to save .fcs files after each step as Rstudio may sometimes crash.*

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
library('glue')
```

```{r}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

#select patientmap you want to preprocess
patient_group <- 'aml' #healthy BM/mds/aml
fcs_dir <- paste0("D:/Internship/data/0. full dataset/", patient_group)

if (!dir.exists(fcs_dir)) stop("Directory does not exist: ", fcs_dir)

#which files are present in the dir
fcs_files <- list.files(path = fcs_dir, pattern = "\\.LMD$", full.names = TRUE)

#extracting files
fcs_data_list <- list()

for (file_path in fcs_files) {
  key <- tools::file_path_sans_ext(basename(file_path))
  
  dataset_index <- NULL
  
  for (i in 1:2) {
    tryCatch({
      ff_try <- suppressWarnings(read.FCS(file_path, dataset = i))
      
      version_keys <- grep("version", names(ff_try@description), ignore.case = TRUE, value = TRUE)
      
      if (length(version_keys) > 0) {
        fcs_version <- ff_try@description[[version_keys[1]]]
        
        if (grepl("^3(\\.0)?$", fcs_version)) {
          fcs_data_list[[key]] <- ff_try
          dataset_index <- i
          break
        }
      }
    }, error = function(e) {
      message(glue("Error reading {basename(file_path)} (dataset {i}): {e$message}"))
    })
  }
  
  if (is.null(dataset_index)) {
    warning(glue("FCS version 3.0 not found for file: {basename(file_path)}"))
  }
}
```

### STEP 1.1: COLUMN NAME STANDARDIZATION

While reading the files, it was observed that column names were not consistent across files. They appeared in two different formats:

```         
[1] "FS-H"   "FS-A"   "SS-H"   "SS-A"   "FL1-A"  "FL2-A"  "FL3-A"  "FL4-A"  "FL5-A"  "FL6-A"  "FL7-A"  "FL8-A"  "FL9-A"  "FL10-A" "TIME" 
```

```         
 [1] "FS INT LIN"   "SS INT LIN"   "FL1 INT LOG"  "FL2 INT LOG"  "FL3 INT LOG"  "FL4 INT LOG"  "FL5 INT LOG"  "FL6 INT LOG"  "FL7 INT LOG"  [10] "FL8 INT LOG"  "FL9 INT LOG"  "FL10 INT LOG" "TIME"         "FS PEAK LIN"  "SS PEAK LIN" 
```

The column names have been standardized to a consistent format to ensure reliable access during bulk preprocessing.

```{r}
#CHECKING COLUMN NAMES BEFORE NAME STANDARDIZATION
fs0 <- fcs_data_list[[2]]
colnames(fs0)
```

```{r}
#STANDARDIZING COLUMNNAMES
rename_common_channels <- function(ff) {
  standard_names <- c("FITC", "PE", "ECD", "PC5.5", "PC7", 
                      "APC", "A700", "A750", "HLA", "KO")
  
  old_names <- colnames(ff)
  new_names <- old_names
  
  #Rename fluorochrome channels
  for (i in seq_along(standard_names)) {
    pattern <- paste0("^FL", i, "([ -].*)?$")
    matched_idx <- grep(pattern, old_names, ignore.case = TRUE)
    
    if (length(matched_idx) > 0) {
      new_names[matched_idx[1]] <- standard_names[i]
    }
  }
  
  #Rename scatter channels
  scatter_renames <- list(
    "FS INT LIN"   = "FS-A",
    "FS PEAK LIN"  = "FS-H",
    "SS INT LIN"   = "SS-A",
    "SS PEAK LIN"  = "SS-H"
  )
  
  for (original in names(scatter_renames)) {
    idx <- which(old_names == original)
    if (length(idx) > 0) {
      new_names[idx] <- scatter_renames[[original]]
    }
  }

  colnames(ff@exprs) <- new_names
  colnames(ff) <- new_names
  
  return(ff)
}

renamed_fcs_list <- lapply(fcs_data_list, rename_common_channels)
```

```{r}
#CHECKING COLUMN NAMES AFTER STANDARDIZATION
fs1 <- renamed_fcs_list[[2]]
colnames(fs1)
```

```{r}
keyword(renamed_fcs_list[[41]])
```

### STEP 1.2: COMPENSATION

Compensation can be done automatically or manually. For this, a **Spillover Spreading Matrix** **(SSM)** is used to correct for the overlap of fluorescent signals between different channels. It is a crucial step as it corrects for fluorescence spillover, where the emission of one fluorochrome is detected in a channel designed for another.

For the new flow cytometer; when using a FCS 2.0 version file, no manual compensation needs to be implemented. For the FCS 3.0 version, manual compensation can be performed by accessing the spillover matrix through `spillover(ff)` and perform compensation by `` compensate(ff, spillover(ff)$`$SPILLOVER`) ``. It is also possible to import the spillover matrix from an excel file and implement it using `compensate(ff, excel_matrix)`

For old flow cytometer; For both versions, no spillover matrices are saved in the .fcs because default compensation matrix from the machine is used. So no manual compensation needs to be performed.

*NOTE: make sure compensation matrix is in fractions and NOT percentages!!*

PS. Columns sometimes contain for FL-7, both the area and width. In the spillover matrix, these two have identical values, resulting in a singular matrix and making it unable to be inverted. This row was therefore removed and these files were then processed.

PPS. In cases where tubes other than Tube 1 do not contain a valid spillover matrix, a reference sample is selected from which the spillover matrix is applied. This ensures proper compensation when the original `.fsc` file lacks a correctly stored matrix.

-   for healthy patient 4 was used as reference

-   for mds patient 103 was used as reference.

-   for aml patient 145 was used as reference.

```{r}
#CHECKING RAW DATA
ff_raw <- renamed_fcs_list[[2]]
df_raw <- as.data.frame(exprs(ff_raw))
```

```{r}
#COMPENSATION

#CHANGE REFERENCE LIST DEPENDING ON DISEASE STATE THAT IS PROCESSED
reference_spillovers <- list(
  "tube 2" = "Ki67-144 tube 2",
  "tube 3" = "Ki67-144 tube 3",
  "tube 4" = "Ki67-144 tube 4",
  "tube 5" = "Ki67-144 tube 5",
  "tube 6" = "Ki67-144 tube 6",
  "tube 7" = "Ki67-144 tube 7"
)

compensate_ff <- function(ff, filename = NULL, fcs_files = NULL, reference_spillovers = NULL) {
  #skip Tube 1
  if (grepl("tube 1", filename, ignore.case = TRUE)) {
    message("Skipping Tube 1: ", filename)
    keyword(ff)[["$TIMESTEP"]] <- 0.000286
    return(ff)
  }
  
  #selecting SSM, otherwise selecting reference SSM
  spill <- tryCatch(spillover(ff)[["$SPILLOVER"]], error = function(e) NULL)
  if (is.null(spill)) {
    tube_label <- sub(".*(tube \\d+).*", "\\1", filename, ignore.case = TRUE)
    tube_label <- trimws(tube_label)
    ref_name <- reference_spillovers[[tube_label]]

    
    if (!is.null(ref_name) && ref_name %in% names(fcs_files)) {
      ref_ff <- fcs_files[[ref_name]]
      spill <- tryCatch(spillover(ref_ff)[["$SPILLOVER"]], error = function(e) NULL)
      message("Using reference spillover from: ", ref_name, " for ", filename)
    }
    
    if (is.null(spill)) {
      warning("No spillover matrix found for: ", filename)
      keyword(ff)[["$TIMESTEP"]] <- 0.000286
      return(ff)
    }
  }
  
  #selecting appropriate channels
  exclude_channels <- c("FS-A", "FS-H", "SS-A", "SS-H", "TIME", "FS TOF LIN", "SS TOF LIN","FL7 INT LIN", "FL7 TOF LIN")
  fluor_channels <- setdiff(colnames(ff), exclude_channels)
  
  colnames(spill) <- fluor_channels
  rownames(spill) <- fluor_channels
  spill_filtered <- spill[setdiff(fluor_channels, "FL7-W"), setdiff(fluor_channels, "FL7-W")]
  
  ff_comp <- compensate(ff, spill_filtered)
  keyword(ff_comp)[["$TIMESTEP"]] <- 0.000286

  return(ff_comp)
}

fcs_compensated <- Map(function(ff, name) {
  compensate_ff(ff, filename = name, fcs_files = renamed_fcs_list, reference_spillovers = reference_spillovers)
}, renamed_fcs_list, names(renamed_fcs_list))
```

```{r}
#CHECKING COMPENSATED DATA
ff_comp <- fcs_compensated[[2]]
df_comp <- as.data.frame(exprs(ff_comp))
```

```{r}
#SAVING COMPENSATED .FCS FILES
output_dir <- normalizePath(
  file.path("D:/Internship/data/2. compensated dataset", patient_group),
  mustWork = FALSE
)
dir.create(output_dir, showWarnings = FALSE)

for (file_name in names(fcs_compensated)) {
  file_path <- file.path(output_dir, paste0(file_name, ".fcs"))
  write.FCS(fcs_compensated[[file_name]], filename = file_path)
}
```

### STEP 1.3 - QUALITY CHECK USING PEACOQC

We first start with remove events whose real intensity exceeds the detector’s sensitivity limits using RemoveMargins from the PeacoQC package.

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
library('PeacoQC')
library('glue')
```

```{r}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

patient_group <- "mds"  #healthy BM/mds/aml
fcs_dir <- file.path("D:/Internship/data/2. compensated dataset", patient_group)


fcs_file_names <- list.files(path = fcs_dir, pattern = "\\.fcs$", full.names = FALSE)
fcs_paths <- file.path(fcs_dir, fcs_file_names)

#named list
fcs_compensated <- setNames(
  lapply(fcs_paths, read.FCS, transformation = FALSE),
  fcs_file_names
)
```

```{r}
#BEFORE QC DATASET SIZES (CAN BE SEEN AS RAW DATA)
raw_data_event_counts <- sapply(fcs_compensated, nrow)

raw_data_event_counts_df <- data.frame(
  Sample = names(raw_data_event_counts),
  Events = raw_data_event_counts
)

write.csv(raw_data_event_counts_df, file = glue("D:/Internship/raw_event_count_{patient_group}.csv"), row.names = FALSE)
```

```{r}
#REMOVE MARGIN EVENTS
channels_of_interest <- c("FS-H", "FS-A", "SS-H", "SS-A", "FITC", "PE", "ECD", "PC5.5","PC7","APC","A700","A750","HLA","KO")

fcs_margins <- lapply(fcs_compensated, function(ff) {
  present_channels <- intersect(channels_of_interest, colnames(exprs(ff)))
  
  PeacoQC::RemoveMargins(ff, present_channels)
})
```

Then we perform the actual quality control.

Quality control is essential to account for potential technical issues that may occur during data acquisition. Examples include sudden signal spikes caused by clogs in the system, gradual signal changes at the start of a run due to instrument warm-up, or fluctuations in acquisition speed that introduce persistent signal shifts. These artifacts can affect a large portion of the data and compromise downstream analysis.

Manually identifying and removing these problematic events is not only time-consuming but also introduces subjectivity. To address this, the Peak Extraction and Cleaning Oriented Quality Control (PeacoQC) can be used. PeacoQC offers a robust, automated approach for detecting and removing low-quality events from `.fcs` files. It scales efficiently for large datasets and demonstrates a high median balanced accuracy compared to other automated quality control methods like flowAI, making it a reliable solution for high-throughput quality control in flow cytometry.

It makes use of the density peaks within your data.

![](images/clipboard-930893002.png)

Once these peaks (regions of low quality events) are isolated, PeacoQC applies a two-step filtering strategy to clean the data:

1.  **Isolation Tree**: This method works similarly to a decision tree. It attempts to isolate abnormal bins based on their peak values, effectively identifying regions with aberrant signal patterns. Isolation trees are particularly useful for detecting abrupt shifts or unusual patterns that affect multiple markers simultaneously. A key advantage is their ability to identify and remove non-consecutive regions of low-quality data (such as short clogging events separated by normal data) which many other methods fail to catch.
2.  **Median Absolute Mediation Distance**: For subtler issues that may not be detected by the isolation tree, PeacoQC uses a second layer of filtering based on Median Absolute Deviation (MAD). This approach smooths the peak values for each marker over time, then calculates how far each bin's smoothed value deviates from the overall median. If a bin deviates beyond a certain threshold (by default, 6 MADs), it is flagged as low quality. This is especially helpful for identifying problems that affect individual markers, such as gradual drifts or isolated channel instability.

PeacoQC then goes on to perform a few important quick checks:

-   It warns you when one of your markers shows consistently increasing or decreasing signal over time. This could indicate problem like contamination. PeacoQC does not remove all the data, it does alert you.

-   It also warns you when it removes more than 70% of your total data. This could be sign of really poor quality sample

*NOTE: TIMESTEP IS NOT PRESENT IN .fcs FILE. In [NAVIOS TETRA MANUAL](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://4.imimg.com/data4/MJ/IL/MY-4372999/navios-flow-cytometer.pdf), the TIME parameter reflects the timing of each event as unitless integers representing time bins, rather than actual seconds. To convert these values into real-time units, TIMESTEP must be applied as scaling factor.*

*For Navios cytometer, TIME channel spans 1,048,576 bins (2²⁰) over a default acquisition period of 300 seconds. Therefore, each TIME unit corresponds to approximately 0.000286 seconds (300 ÷ 1,048,576). Multiplying TIME values by this TIMESTEP converts them into meaningful timestamps, enabling proper time-based quality control and analysis.*

```{r}
for (i in seq_along(fcs_margins)) {
  ff_name <- names(fcs_margins)[i]
  ff_one <- fcs_margins[[i]]
  
  #select channels I want to clean
  present_channels <- colnames(exprs(ff_one))
  channels_to_use <- intersect(channels_of_interest, present_channels)
  missing <- setdiff(channels_of_interest, present_channels)
  if (length(missing) > 0) {
    warning(paste("In", ff_name, "missing channels:", paste(missing, collapse = ", ")))
  }

  message(paste("Running PeacoQC on:", ff_name, "using channels:", paste(channels_to_use, collapse = ", ")))

  peacoqc_res <- PeacoQC(
    ff = ff_one,
    channels = channels_to_use,
    determine_good_cells = "all",
    save_fcs = TRUE,
    plot = TRUE,
    output_directory = file.path("D:/Internship/data/3. cleaned dataset", patient_group)
  )
}
```

### STEP 1.4 - TRANSFORMATION

Compensation corrects for spectral overlap between fluorochromes but does not address the underlying distribution of flow cytometry data. Such data typically exhibits a positively skewed distribution, especially in fluorescent channels. To make the data more suitable for analysis and visualization, transformation methods—such as logarithmic, biexponential (logicle), or arcsinh—are applied to achieve a more symmetric, near-normal distribution.

In this workflow, fluorescent channels undergo an arcsinh transformation, while scatter channels are transformed linearly. After these transformations, all channels are normalized using min–max scaling, following the approach described by Duetz et al. (2021).

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
library('PeacoQC')
```

```{r}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

patient_group <- "mds"  #healthy BM/mds/aml
fcs_dir <- file.path("D:/Internship/data/3. cleaned dataset", patient_group)


fcs_file_names <- list.files(path = fcs_dir, pattern = "\\.fcs$", full.names = FALSE)
fcs_paths <- file.path(fcs_dir, fcs_file_names)

#named list
fcs_cleaned <- setNames(
  lapply(fcs_paths, read.FCS, transformation = FALSE),
  fcs_file_names
)
```

#### Transformations

```{r}
#ARCSINH TRANSFORMATION
arcsinh_transform <- function(ff, cofactor = 150, exclude_channels = c("FS-A", "FS-H", "SS-A", "SS-H", "TIME")) {
  transform_channels <- setdiff(colnames(exprs(ff)), exclude_channels)
  
  arcsinh_tf <- transformList(
    transform_channels,
    arcsinhTransform(a = 0, b = 1 / cofactor, c = 0)
  )

  ff_transformed <- transform(ff, arcsinh_tf)
  
  return(ff_transformed)
}

fcs_transformed <- lapply(fcs_cleaned, arcsinh_transform, cofactor = 150)
```

For the linear transformation, a reference marker is needed. We used CD45 marker as the reference.

```{r}
linear_transform <- function(flow_frame, reference_marker = "KO",
                                       scatter_channels = c("SS-A", "FS-A", "SS-H", "FS-H")) {
  expr <- exprs(flow_frame)

  #checking if channels are present
  missing <- setdiff(c(reference_marker, scatter_channels), colnames(expr))
  if (length(missing) > 0) {
    warning(paste("Skipping file: missing channels:", paste(missing, collapse = ", ")))
    return(flow_frame)
  }

  #Applying linear transformation
  for (channel in scatter_channels) {
    q5_ref  <- quantile(expr[, reference_marker], 0.05, na.rm = TRUE)
    q95_ref <- quantile(expr[, reference_marker], 0.95, na.rm = TRUE)
    q5_sc   <- quantile(expr[, channel], 0.05, na.rm = TRUE)
    q95_sc  <- quantile(expr[, channel], 0.95, na.rm = TRUE)

    a <- (q95_ref - q5_ref) / (q95_sc - q5_sc)
    b <- q5_ref - a * q5_sc

    lin_trans <- transformList(channel, linearTransform(a = a, b = b))
    flow_frame <- transform(flow_frame, lin_trans)
  }

  return(flow_frame)
}

fcs_transform_lin <- lapply(fcs_transformed, linear_transform)

# Preserve original names
names(fcs_transform_lin) <- names(fcs_transformed)
```

#### Min-Max Normalization

For the min-max normalization the 0.001 and 0.999 quantiles are chosen as this is more robust against outliers, avoiding accidental skewing of the data if a few extreme measurements would be included.

```{r}
#MIN-MAX NORMALIZATION
min_max_quantile_normalize <- function(ff, exclude_channels = c("TIME", 'Original_ID')) {
  expr <- exprs(ff)
  
  norm_channels <- setdiff(colnames(expr), exclude_channels)
  
  expr[, norm_channels] <- apply(expr[, norm_channels, drop = FALSE], 2, function(x) {
    (x - quantile(x, 0.001, na.rm = TRUE)) / 
      (quantile(x, 0.999, na.rm = TRUE) - quantile(x, 0.001, na.rm = TRUE))
  })
  
  flowCore::exprs(ff) <- expr
  return(ff)
}

fcs_norm <- vector("list", length(fcs_transformed))
names(fcs_norm) <- names(fcs_transformed)

for (i in seq_along(fcs_transformed)) {
  cat("Normalizing flowFrame", i, "of", length(fcs_transformed), "\n")
  fcs_norm[[i]] <- min_max_quantile_normalize(fcs_transformed[[i]])
}
```

```{r}
#EVENT COUNT AFTER QUALITY CONTROL AND TRANSFORMATION
QC_data_event_counts <- sapply(fcs_norm, nrow)

QC_data_event_counts_df <- data.frame(
  Sample = names(QC_data_event_counts),
  Events = QC_data_event_counts
)

write.csv(QC_data_event_counts_df, file = "D:/Internship/afterQC_event_count_mds.csv", row.names = FALSE)
```

```{r}
output_dir <- normalizePath(
  file.path("D:/Internship/data/4. transformed dataset", patient_group),
  mustWork = FALSE
)
dir.create(output_dir, showWarnings = FALSE)

for (file_name in names(fcs_norm)) {
  file_path <- file.path(output_dir, paste0(file_name, ".fcs"))
  write.FCS(fcs_norm[[file_name]], filename = file_path)
}
```

## STEP 2: REMOVAL OF UNWANTED EVENTS

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
library(dplyr)
```

```{r}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

patient_group <- "healthy BM"  #healthy BM/mds/aml
fcs_dir <- file.path("D:/Internship/data/4. transformed dataset", patient_group)


fcs_file_names <- list.files(path = fcs_dir, pattern = "\\.fcs$", full.names = FALSE)
fcs_paths <- file.path(fcs_dir, fcs_file_names)

#named list
fcs_transformed <- setNames(
  lapply(fcs_paths, read.FCS, transformation = FALSE),
  fcs_file_names
)
```

#### Visualization before gating is applied

```{r}
output_dir <- normalizePath(
  file.path("D:/Internship/data/5. preprocessed dataset", patient_group, "/before_gating/FSCvsSSC"),
  mustWork = FALSE
)
dir.create(output_dir, showWarnings = FALSE)

for (name in names(fcs_transformed)) {
  raw_ff <- fcs_transformed[[name]]
  base_name <- tools::file_path_sans_ext(name)
  
  p <- ggcyto(raw_ff, aes(x = `FS-A`, y = `SS-A`)) +
    geom_hex(bins = 128) +
    labs(
      title = paste("FSC-A vs SSC-A - (before gating):", base_name),
      x = "FSC-A", y = "SSC-A"
    ) +
    theme_minimal() +
    scale_fill_viridis_c(option = "C", direction = -1)
  
  ggsave(
    filename = file.path(output_dir, paste0(base_name, "_FSC-A_vs_SSC-A.png")),
    plot = p,
    width = 5, height = 5, dpi = 300
  )
}
```

### STEP 2.1 - GATING OUT DEBRIS

In order to gate out debris, the function `flowClust.2d` is employed. This performs automated gating by applying model-based clustering FS-A and SS-A channels. It uses the `flowClust` algorithm, which fits a mixture of multivariate t-distributions to the selected channels, modeling the data as a combination of distinct cell populations (clusters).

In this case 2 clusters are chosen, one for debris and the other for cells. `flowClust.2d` then identifies the main cluster of interest (usually the largest or most biologically relevant cluster) based on membership probabilities. It converts this cluster's boundary into a polygon gate using the `clust2Poly` function, which defines the spatial region of this population in the 2D parameter space.

This approach provides a statistically rigorous, unsupervised way to separate cell populations from debris or outliers based on scatter properties, allowing for consistent and reproducible gating prior to downstream analysis.

Applying the clustering algorithm directly to the full dataset was found to be overly aggressive and often resulted in the exclusion of a substantial number of valid events. So, the algorithm was applied within this subregion and the cluster identified as representing debris was removed from the full dataset.

```{r}
# Helper function
round_down_to_1_followed_by_zeros <- function(x) {
  if (x <= 0) return(1)
  10^floor(log10(x))
}

fcs_gated <- vector("list", length(fcs_transformed))

#ACTUAL GATING
for (i in seq_along(fcs_transformed)) {
  cat("Processing sample", i, "of", length(fcs_transformed), "\n")
  
  ff <- fcs_transformed[[i]]
  expr <- exprs(ff)[, c("FS-A", "SS-A")]

  max_fs <- max(expr[, "FS-A"], na.rm = TRUE)
  max_ss <- max(expr[, "SS-A"], na.rm = TRUE)
  
  max_fs_rounded <- round_down_to_1_followed_by_zeros(max_fs)
  max_ss_rounded <- round_down_to_1_followed_by_zeros(max_ss)
  
  #gating out debris
  debris_gate_limits <- list(
    "FS-A" = c(0, (max_fs_rounded / 4)),
    "SS-A" = c(0, (max_ss_rounded / 4))
  )
  
  ff_rect <- Subset(ff, rectangleGate(filterId = "DebrisRect", .gate = debris_gate_limits))
  
  fs <- flowSet(list(sample1 = ff_rect))
  
  debris_gate <- fsApply(fs, function(fr) {
    openCyto:::.flowClust.2d(
      fr,
      channels = c("FS-A", "SS-A"),
      K = 2,
      target = c(25000, 25000),
      filterId = "flowClustDebris"
    )
  })
  
  
  ff_clean <- Subset(ff, !debris_gate[[1]])
  
  #Triangle gate to remove possible fat artifacts
  triangle_coords <- matrix(
    c(
      0, max_ss,                            #top left
      ((max_fs / 16) * 7), max_ss,          #top right
      0, 0                                  #bottom corner
    ),
    ncol = 2,
    byrow = TRUE,
    dimnames = list(NULL, c("FS-A", "SS-A"))
  )
  
  triangle_gate <- polygonGate(filterId = "TriangleGate", .gate = triangle_coords)
  
  ff_clean <- Subset(ff_clean, !triangle_gate)

  # Final strict debris exclusion using rectangle
  debris_box_gate <- rectangleGate(
  filterId = "DebrisLowerLeftBox",
  "FS-A" = c(-Inf, (max_fs_rounded / 8)),
  "SS-A" = c(-Inf, (max_ss_rounded / 8))
)
  ff_clean <- Subset(ff_clean, !debris_box_gate)

  fcs_gated[[i]] <- ff_clean
}
```

```{r}
#AFTER GATING DATASET SIZES
names(fcs_gated) <- basename(fcs_file_names)

gated_data_event_counts <- sapply(fcs_gated, nrow)
gated_data_event_counts_df <- data.frame(
  Sample = names(gated_data_event_counts),
  Events = gated_data_event_counts
)

write.csv(gated_data_event_counts_df, file = "D:/Internship/gated_event_count_mds.csv", row.names = FALSE)
```

### STEP 2.2 - REMOVING DOUBLETS

We can use a developed R code section from the Duetz et al. (2021) paper to remove doublets from the data. This was manually adapted to filter both 2\*std above and below the expected values.

```{r}
is_single <- function(ff, plot = FALSE, ...) {
  fsc_a <- flowCore::exprs(ff)[,"FS-A"]
  fsc_h <- flowCore::exprs(ff)[,"FS-H"]
  
  bins <- cut(fsc_a, 10)
  
  ratios <- fsc_h / fsc_a
  slope_per_bin <- tapply(ratios, bins, mean)
  expected_values <- fsc_a * slope_per_bin[bins]
  deviations <- abs(fsc_h - expected_values)
  
  x <- tapply(fsc_a, bins, mean)
  e <- tapply(expected_values, bins, mean)
  d <- tapply(deviations, bins, function(x){mean(x) + 2*sd(x)})
  
  lower_y <- e - d
  upper_y <- e + d
  
  lower_spl <- splinefun(x, lower_y)
  upper_spl <- splinefun(x, upper_y)
  
  selection <- fsc_h > lower_spl(fsc_a) & fsc_h < upper_spl(fsc_a)
  return(selection)
}

single_cells_list <- vector("list", length(fcs_gated))
names(single_cells_list) <- names(fcs_gated)

for (i in seq_along(fcs_gated)) {
  cat(sprintf("Processing sample %d of %d: %s\n", i, length(fcs_gated), names(fcs_gated)[i]))
  single_cells_list[[i]] <- is_single(fcs_gated[[i]], plot = FALSE)
}

fcs_singles <- mapply(function(ff, sel) {
  ff[sel, ]  # keep rows where sel == TRUE
}, fcs_gated, single_cells_list, SIMPLIFY = FALSE)
```

```{r}
#AFTER DOUBLET REMOVAL DATASET SIZES
single_data_event_counts <- sapply(fcs_singles, nrow)
single_data_event_counts_df <- data.frame(
  Sample = names(single_data_event_counts),
  Events = single_data_event_counts
)

write.csv(single_data_event_counts_df, file = "D:/Internship/preprocessed_event_count_mds.csv", row.names = FALSE)
```

#### Visualization after gating is applied

```{r}
output_dir <- normalizePath(
  file.path("D:/Internship/data/5. preprocessed dataset", patient_group, "/after_gating/CD45vSSC-A"),
  mustWork = FALSE
)
dir.create(output_dir, showWarnings = FALSE)

for (name in names(fcs_singles)) {
  raw_ff <- fcs_singles[[name]]
  base_name <- tools::file_path_sans_ext(name)
  
  p <- ggcyto(raw_ff, aes(x = `KO`, y = `SS-A`)) +
    geom_hex(bins = 128) +
    labs(
      title = paste("CD45 vs SSC-A - (after gating):", base_name),
      x = "CD45", y = "SSC-A"
    ) +
    theme_minimal() +
    scale_fill_viridis_c(option = "C", direction = -1)
  
  ggsave(
    filename = file.path(output_dir, paste0(base_name, "_CD45_vs_SSC-A.png")),
    plot = p,
    width = 5, height = 5, dpi = 300
  )
}
```

### STEP 2.3 - SAVING PREPROCESSED FILES

```{r}
library(flowCore)

#output folder
output_dir <- normalizePath(
  file.path("D:/Internship/data/5. preprocessed dataset", patient_group),
  mustWork = FALSE
)
dir.create(output_dir, showWarnings = FALSE)

for (name in names(fcs_singles)) {
  ff <- fcs_singles[[name]]
  filename <- file.path(output_dir, paste0(tools::file_path_sans_ext(name)))
  write.FCS(ff, filename)
  cat("Saved:", filename, "\n")
}
```

## STEP 3: AGGREGATE FILES

We aggregate files per tube for all patients within one disease group. This way we end up with 7 .fcs files representing the 7 tubes measured for that specific disease state. This aggregation step was necessary, as the FlowSOM algorithm requires a single .fcs file as input for training and clustering.

The original paper opted for a method that selected 40.000 cells per .fcs, however this does not account for differences in .fcs sizes. So what we did here is calculate how many events we want in the final aggregated file and based on that calculation we use the function `AggregateFlowFrames` to select the events from all the patients. This gives the most robust results.

-   for Healthy group: 40.000 x 50 = 2.000.000

-   for MDS patients: 40.000 x 25 = 1.000.000

-   for AML patients: 40.000 x 18 = 720.000

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
library(dplyr)
```

```{r}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

patient_group <- "healthy BM"  #healthy BM/mds/aml

fcs_dir <- file.path("D:/Internship/data/5. preprocessed dataset", patient_group)
output_dir <- normalizePath(
  file.path("D:/Internship/data/6. aggregated files", patient_group),
  mustWork = FALSE
)
dir.create(output_dir, showWarnings = FALSE)

all_fcs_files <- list.files(path = fcs_dir, pattern = "\\.fcs$", full.names = FALSE)

set.seed(2020)

aggregate_cells <- 2000000
tubes <- paste0("tube ", 1:7)
```

```{r}
for (tube in tubes) {
  cat("🔄 Processing", tube, "...\n")
  
  tube_file_names <- all_fcs_files[grepl(tube, all_fcs_files, ignore.case = TRUE)]
  
  if (length(tube_file_names) == 0) {
    warning(paste("⚠️ No files found for", tube, "- skipping."))
    next
  }
  
  tube_file_paths <- file.path(fcs_dir, tube_file_names)
  
  #perform aggregation per tube
  agg <- FlowSOM::AggregateFlowFrames(
    fileNames = tube_file_paths,
    cTotal = aggregate_cells,
    writeOutput = TRUE,
    outputFile = file.path(output_dir, paste0(tube, "_aggregate.fcs"))
  )
  
  cat("✅ Aggregated", length(tube_file_paths), "files for", tube, "\n")
}
```

### FIXING THE NAMES OF THE COLUMNS TO WHAT THEY REPRESENT

```{r}
#REMOVING COLUMNS THAT WILL NOT BE USED AND RENAME COLUMNS ACCORDINGLY
ff <- read.FCS(file.path("D:/Internship/data/6. aggregated files", patient_group, 'tube 7_aggregate.fcs'))

colnames(ff)
```

```{r}
cols_to_remove <- c("FL7-W", 'Original_ID', 'File_scattered', 'File', 'Original_ID2')
ff_clean <- ff[, !(colnames(ff) %in% cols_to_remove)]
colnames(ff_clean)
#RENAMING COLUMNS PER TUBE TO WHAT THEY REPRESENT
```

These are the names of the columns per tube:

-   **Tube 1:** c( "FSC-A", "SSC-A", "IgG1-1", "NA1", "IgG1-2", "CD13", "CD117", "CD34", "NA2", "NA3", "HLA-DR", "CD45","TIME","FSC-H", "SSC-H")

-   **Tube 2:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "Ki-67", "CD14", "CD64", "CD13", "CD117", "CD34", "CD10", "CD11b", "HLA-DR", "CD45","TIME")

-   **Tube 3:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "Ki-67", "CD105", "CD123", "CD33", "CD117", "NA1", "CD71", "CD235a", "HLA-DR", "CD45","TIME")

-   **Tube 4:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A","Ki-67", "NA1","NA2","CD33", "CD117", "CD36", "NA4", "NA5","HLA-DR", "CD45", "TIME")

-   **Tube 5:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "CD16", "CD64", "Bcl-2", "CD13", "CD117", "CD34", "CD10", "CD14", "HLA-DR", "CD45","TIME")

-   **Tube 6:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "NA1", "CD105", "Bcl-2", "CD33", "CD117", "NA2", "CD71", "CD235a", "HLA-DR", "CD45","TIME")

-   **Tube 7:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "NA1", "NA2", "Bcl-2", "CD33", "CD117", "CD36", "NA3","NA4", "HLA-DR", "CD45", "TIME")

NOTE: order of columns can change per tube, check this manually!!!

```{r}
colnames(ff_clean) <- c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "NA1", "NA2", "Bcl-2", "CD33", "CD117", "CD36", "NA3","NA4", "HLA-DR", "CD45", "TIME")

write.FCS(ff_clean, filename = file.path("D:/Internship/data/6. aggregated files", patient_group, 'tube 7_final.fcs'))
```

# FLOWSOM

## STEP 1: LOADING IN DATA + LIBRARIES

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
library(dplyr)
```

```{r}
patient_group <- 'healthy BM' #healthy BM/mds/aml
ff <- read.FCS(file.path("D:/Internship/data/6. aggregated files", patient_group, 'tube 2_final.fcs'))
colnames(ff)
```

## STEP 2: RUNNING FLOWSOM

You start of with building a tree based on the lineage markers within your panel. This is done to distinguish cell populations within your dataset.

### STEP 2.1: Cell population Tree

selection of channels:

-   **Tube 1:** c("SSC-A", "IgG1-1", "IgG1-2", "CD13", "CD117", "CD34", "HLA-DR", "CD45")

-   **Tube 2:** c("SSC-A", "CD14", "CD64", "CD13", "CD117", "CD34", "CD10", "CD11b", "HLA-DR", "CD45")

-   **Tube 3:** c("SSC-A", "CD105", "CD123", "CD33", "CD117", "CD71", "CD235a", "HLA-DR", "CD45")

-   **Tube 4:** c("SSC-A","CD33", "CD117", "CD36","HLA-DR", "CD45")

-   **Tube 5:** c("SSC-A", "CD16", "CD64", "Bcl-2", "CD13", "CD117", "CD34", "CD10", "CD14", "HLA-DR", "CD45")

-   **Tube 6:** c("SSC-A", "CD105", "CD33", "CD117", "CD71", "CD235a", "HLA-DR", "CD45")

-   **Tube 7:** c("SSC-A","CD33", "CD117", "CD36", "HLA-DR", "CD45")

```{r}
dir_results <- file.path("D:/Internship/FlowSOM results", patient_group)
dir.create(dir_results, showWarnings = FALSE)

#parameters are here chosen by https://www.youtube.com/watch?v=hP1aTZ1Iqfcthe 10x10 grid most used for smaller panels. 12x12, 15x15 or 20x20 often chosen for larger panels. for metaclusters, recommendation is to choose 1.5 times the amount of clusters you are expecting. Use the scatterplots to finetune. The algorithms to choose the amount of clustering within flowSOM are not recommended as they strongly underestimate the amount of clusters. 
SOM_x <- 12 
SOM_y <- 12 
n_meta <- 12 
seed <- 2020 
scaling <- FALSE

#extracting relevant columns
expr_matrix <- exprs(ff)

#tube 1: 
selected_exprs <- expr_matrix[, c("SSC-A", "Ki-67", "CD14", "CD64", "CD13", "CD117", "CD34", "CD10", "CD11b", "HLA-DR", "CD45")] #define which columns

ff_selected <- flowFrame(selected_exprs)
```

```{r}
library(FlowSOM)
fsom <- FlowSOM(input = ff_selected, scale = scaling, 
                seed = seed, 
                nClus = n_meta, 
                xdim = SOM_x, 
                ydim = SOM_y)
```

Plot the FlowSOM tree structure to visualize the clustering results. The `PlotStars` function generates a star plot, which is a common way to visualize FlowSOM results, showing the clusters and their relationships.

```{r}
p <- PlotStars(fsom = fsom, backgroundValues = fsom$metaclustering, maxNodeSize=1.5, equalNodeSize=TRUE)
ggsave(filename = paste0(dir_results, "fsom_tree_tube2.pdf"), plot = p, height = 8.5, width = 11)
```

Create a report summarizing the FlowSOM results, including cluster information and visualizations. The `FlowSOMmary` function generates a comprehensive report that can be saved as a PDF file.

```{r}
FlowSOMmary(fsom, plotFile = paste0(dir_results, "fsom_summary_tube2.pdf"))
```

### Inspecting cells in 2D scatterplots

A good way to determine the cluster quality of the chosen parameters, we can look at the scatterplots.

```{r}
#SCATTERPLOTS
channel_pairs = list(c("CD45", "SSC-A")) 

metaclusters_of_interest <- seq_len(n_meta)
clusters_of_interest <- NULL

Plot2DScatters(fsom = fsom, channelpairs = channel_pairs, metaclusters = metaclusters_of_interest, clusters = clusters_of_interest, plotFile = paste0(dir_results, "fsom_2D_scatters_tube2.png"), centers = FALSE, density=FALSE)
```
### STEP 2.2: Expression of functional markers for tree

```{r}
p2 <- PlotMarker(fsom = fsom, marker='Ki-67', colorPalette = grDevices::colorRampPalette(c(
  "#00007F", "blue", "#007FFF", "cyan", "red", "#7F0000"
)))
ggsave(filename = paste0(dir_results, "fsom_tree_tube2_whsjoisjowjswswswswsws.pdf"), plot = p2, height = 8.5, width = 11)
```

