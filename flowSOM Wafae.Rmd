---
title: "Protocol for FlowSOM implementation"
output: html_notebook
---

NOTE: *R version 4.5.0*. Also make sure [Rtools45](https://cran.r-project.org/bin/windows/Rtools/rtools45/rtools.html) is downloaded.

## STEP 0.1: DOWNLOADING PACKAGES (SKIP IF ALREADY DOWNLOADED)

Navigate towards 'Tools \> Install Packages' and download the following packages:

1.  *ggplot2*
2.  *ggpubr*
3.  *pheatmap*
4.  *tidyr*
5.  *devtools*

Then go on to download *BioConductor* in the following way in order to download *FlowCore* correctly.

```{r}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

#The following initializes usage of Bioc devel
BiocManager::install(version='devel')
BiocManager::install("flowCore", force = TRUE)
BiocManager::install("flowAI")
BiocManager::install("ggcyto")
BiocManager::install("openCyto")

#use devtools to download packages from github
devtools::install_github("saeyslab/PeacoQC", force=TRUE)
devtools::install_github("saeyslab/FlowSOM")
```

## STEP 1: PREPROCESSING: STANDARDIZATION + TRANSFORMATION STEPS.

The code below selects the patient files based on their patient_id from the .LMD files. The .LMD files contain two versions of .fcs files: Version 3.0 or Version 2.0. In the code below version 3.0 is selected as these contain the spill over matrices for compensation.

*NOTE: preprocessing full dataset can be heavy for CPU, better to preprocess in batches. + good to save .fcs files after each step as Rstudio may sometimes crash*

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
```

```{r}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

fcs_dir <- "D:/Internship/data/full dataset/"

#parsing filenames and setting up patient dictionary
original_ds <- list.files(path=fcs_dir, pattern= '\\.LMD$', full.names=FALSE)
patient_ids <- as.integer(sub(".*[^0-9](\\d+)\\s+tube.*", "\\1", original_ds))
patient_file_dict <- split(original_ds, patient_ids)

#select which patients you want to extract: AML, MDS, healthy or full dataset
aml_patients = c(144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 196, 197, 198, 199, 201)

#mds_patients = c(103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 186, 187, 188, 189, 192, 195, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211)

#healthy_patients = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50)

#full_dataset = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50,103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 186, 187, 188, 189, 192, 195, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 196, 197, 198, 199, 201)

if (exists("aml_patients")) {
  selected_patients <- aml_patients
} else if (exists("mds_patients")) {
  selected_patients <- mds_patients
} else if (exists("healthy_patients")) {
  selected_patients <- healthy_patients
} else if (exists("full_dataset")) {
  selected_patients <- full_dataset
} else {
  stop("No patient group was defined.")
}


fcs_files <- unlist(patient_file_dict[as.character(selected_patients)])
fcs_data_list <- list()

for (id in names(patient_file_dict)) {
  if (as.integer(id) %in% selected_patients) {
    file_names <- patient_file_dict[[id]]
    
    for (file in file_names) {
      file_path <- file.path(fcs_dir, file)
      key <- tools::file_path_sans_ext(file)
      
      dataset_index <- NULL
      
      for (i in 1:2) {
        tryCatch({
          ff_try <- suppressWarnings(read.FCS(file_path, dataset = i))
          
          version_keys <- grep("version", names(ff_try@description), ignore.case = TRUE, value = TRUE)
          
          if (length(version_keys) > 0) {
            fcs_version <- ff_try@description[[version_keys[1]]]
            
            if (grepl("^3(\\.0)?$", fcs_version)) {
              fcs_data_list[[key]] <- ff_try
              dataset_index <- i
              break
            }
          }
        }, error = function(e) {})
      }
      
      if (is.null(dataset_index)) {
        warning(paste("FCS version 3.0 not found for file:", file))
      }
    }
  }
}
```

### STEP 1.1: COLUMN NAME STANDARDIZATION

While reading the files, it was observed that column names were not consistent across files. They appeared in two different formats:

```         
[1] "FS-H"   "FS-A"   "SS-H"   "SS-A"   "FL1-A"  "FL2-A"  "FL3-A"  "FL4-A"  "FL5-A"  "FL6-A"  "FL7-A"  "FL8-A"  "FL9-A"  "FL10-A" "TIME" 
```

```         
 [1] "FS INT LIN"   "SS INT LIN"   "FL1 INT LOG"  "FL2 INT LOG"  "FL3 INT LOG"  "FL4 INT LOG"  "FL5 INT LOG"  "FL6 INT LOG"  "FL7 INT LOG"  [10] "FL8 INT LOG"  "FL9 INT LOG"  "FL10 INT LOG" "TIME"         "FS PEAK LIN"  "SS PEAK LIN" 
```

The column names have been standardized to a consistent format to ensure reliable access during bulk preprocessing.

```{r}
#CHECKING COLUMN NAMES BEFORE NAME STANDARDIZATION
fs0 <- fcs_data_list[[2]]
colnames(fs0)
```

```{r}
#STANDARDIZING COLUMNNAMES
rename_common_channels <- function(ff) {
  standard_names <- c("FITC", "PE", "ECD", "PC5.5", "PC7", 
                      "APC", "A700", "A750", "HLA", "KO")
  
  old_names <- colnames(ff)
  new_names <- old_names
  
  #Rename fluorochrome channels
  for (i in seq_along(standard_names)) {
    pattern <- paste0("^FL", i, "([ -].*)?$")
    matched_idx <- grep(pattern, old_names, ignore.case = TRUE)
    
    if (length(matched_idx) > 0) {
      new_names[matched_idx[1]] <- standard_names[i]
    }
  }
  
  #Rename scatter channels
  scatter_renames <- list(
    "FS INT LIN"   = "FS-A",
    "FS PEAK LIN"  = "FS-H",
    "SS INT LIN"   = "SS-A",
    "SS PEAK LIN"  = "SS-H"
  )
  
  for (original in names(scatter_renames)) {
    idx <- which(old_names == original)
    if (length(idx) > 0) {
      new_names[idx] <- scatter_renames[[original]]
    }
  }

  colnames(ff@exprs) <- new_names
  colnames(ff) <- new_names
  
  return(ff)
}

renamed_fcs_list <- lapply(fcs_data_list, rename_common_channels)
```

```{r}
#CHECKING COLUMN NAMES AFTER STANDARDIZATION
fs1 <- renamed_fcs_list[[2]]
colnames(fs1)
```

### STEP 1.2: COMPENSATION

Compensation can be done automatically or manually. For this, a **Spillover Spreading Matrix**Â **(SSM)** is used to correct for the overlap of fluorescent signals between different channels. It is a crucial step as it corrects for fluorescence spillover, where the emission of one fluorochrome is detected in a channel designed for another.

For the new flow cytometer; when using a FCS 2.0 version file, no manual compensation needs to be implemented. For the FCS 3.0 version, manual compensation can be performed by accessing the spillover matrix through `spillover(ff)` and perform compensation by `` compensate(ff, spillover(ff)$`$SPILLOVER`) ``. It is also possible to import the spillover matrix from an excel file and implement it using `compensate(ff, excel_matrix)`

For old flow cytometer; For both versions, no spillover matrices are saved in the .fcs because default compensation matrix from the machine is used. So no manual compensation needs to be performed.

*NOTE: make sure compensation matrix is in fractions and NOT percentages!!*

PS. Columns sometimes contain for FL-7, both the area and width. In the spillover matrix, these two have identical values, resulting in a singular matrix and making it unable to be inverted. This row was therefore removed and these files were then processed.

PPS. In cases where tubes other than Tube 1 do not contain a valid spillover matrix, a reference sample is selected from which the spillover matrix is applied. This ensures proper compensation when the original `.fsc` file lacks a correctly stored matrix.

```{r}
#CHECKING RAW DATA
ff_raw <- renamed_fcs_list[[2]]
df_raw <- as.data.frame(exprs(ff_raw))
```

```{r}
#COMPENSATION

#CHANGE REFERENCE LIST DEPENDING ON DISEASE STATE THAT IS PROCESSED
reference_spillovers <- list(
  "tube 2" = "Ki67-144 tube 2",
  "tube 3" = "Ki67-144 tube 3",
  "tube 4" = "Ki67-144 tube 4",
  "tube 5" = "Ki67-144 tube 5",
  "tube 6" = "Ki67-144 tube 6",
  "tube 7" = "Ki67-144 tube 7"
)

compensate_ff <- function(ff, filename = NULL, fcs_files = NULL, reference_spillovers = NULL) {
  #skip Tube 1
  if (grepl("tube 1", filename, ignore.case = TRUE)) {
    message("Skipping Tube 1: ", filename)
    keyword(ff)[["$TIMESTEP"]] <- 0.000286
    return(ff)
  }
  
  #selecting SSM, otherwise selecting reference SSM
  spill <- tryCatch(spillover(ff)[["$SPILLOVER"]], error = function(e) NULL)
  if (is.null(spill)) {
    tube_label <- sub(".*(tube \\d+).*", "\\1", filename, ignore.case = TRUE)
    tube_label <- trimws(tube_label)
    ref_name <- reference_spillovers[[tube_label]]

    
    if (!is.null(ref_name) && ref_name %in% names(fcs_files)) {
      ref_ff <- fcs_files[[ref_name]]
      spill <- tryCatch(spillover(ref_ff)[["$SPILLOVER"]], error = function(e) NULL)
      message("Using reference spillover from: ", ref_name, " for ", filename)
    }
    
    if (is.null(spill)) {
      warning("No spillover matrix found for: ", filename)
      keyword(ff)[["$TIMESTEP"]] <- 0.000286
      return(ff)
    }
  }
  
  #selecting appropriate channels
  exclude_channels <- c("FS-A", "FS-H", "SS-A", "SS-H", "TIME", "FS TOF LIN", "SS TOF LIN","FL7 INT LIN", "FL7 TOF LIN")
  fluor_channels <- setdiff(colnames(ff), exclude_channels)
  
  colnames(spill) <- fluor_channels
  rownames(spill) <- fluor_channels
  
  spill_filtered <- spill[setdiff(fluor_channels, "FL7-W"), setdiff(fluor_channels, "FL7-W")]
  
  ff_comp <- compensate(ff, spill_filtered)
  keyword(ff_comp)[["$TIMESTEP"]] <- 0.000286

  return(ff_comp)
}

fcs_compensated <- Map(function(ff, name) {
  compensate_ff(ff, filename = name, fcs_files = renamed_fcs_list, reference_spillovers = reference_spillovers)
}, renamed_fcs_list, names(renamed_fcs_list))
```

```{r}
#CHECKING COMPENSATED DATA
ff_comp <- fcs_compensated[[2]]
df_comp <- as.data.frame(exprs(ff_comp))
```

```{r}
#SAVING COMPENSATED .FCS FILES
output_dir <- normalizePath("D:/Internship/data/compensated dataset", mustWork = FALSE)
dir.create(output_dir, showWarnings = FALSE)

for (file_name in names(fcs_compensated)) {
  file_path <- file.path(output_dir, paste0(file_name, ".fcs"))
  write.FCS(fcs_compensated[[file_name]], filename = file_path)
}
```

### STEP 1.3 - QUALITY CHECK USING FLOWAI

flowAI is used to remove events having anomalous values when looking at three aspect of a flow cytometry analysis:

1.  **flow rate**: Checks whether cells are passing through the flow cytometer at an appropriate rate. Anomalies could be due to clumping of cells, clogging or incorrect flow rate settings. A high percentages suggests that cells may be analyzed under suboptimal flow conditions.

    -   This step uses *generalized ESD* to detect outliers. See [this link](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h3.htm) for further explanation of this test.

2.  **signal acquisition**: accounts for fluorescence and scatter signal of cells. Anomalies can arise due to inadequate staining, photobleaching etc. A substantial proportion of cells with anomalous signal acquisition can lead to inaccurate fluorescence intensity measurements, affecting the interpretation of the data.

    -   Uses *Binary Segmentation* to check for changepoints and remove instable regions

3.  **dynamic range**: Evaluates whether intensities of cells fall within detectable range of cytometer. Cells outside this range can either be too dim (below detection limit) or too bright (causing signal saturation), both of which can skew data interpretation.

    -   Filter out all measurements above hard upper limit and removes all negative scatter values. For fluorescent channels, it removed outliers of negative range.

Several arguments can be changed within `flow_auto_qc` to improve quality control results. Moreover, `remove_from` can be used to perform partial quality control on only one or two of the above mentioned properties.

The suggestion is to run automatic method first with default settings, if results are not satisfying, we can modify arguments.

*NOTE: TIMESTEP IS NOT PRESENT IN .fcs FILE. In [NAVIOS TETRA MANUAL](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://4.imimg.com/data4/MJ/IL/MY-4372999/navios-flow-cytometer.pdf), the TIME parameter reflects the timing of each event as unitless integers representing time bins, rather than actual seconds. To convert these values into real-time units, TIMESTEP must be applied as scaling factor.*

*For Navios cytometer, TIME channel spans 1,048,576 bins (2Â²â°) over a default acquisition period of 300 seconds. Therefore, each TIME unit corresponds to approximately 0.000286 seconds (300 Ã· 1,048,576). Multiplying TIME values by this TIMESTEP converts them into meaningful timestamps, enabling proper time-based quality control and analysis.*

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
```

```{r}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

fcs_dir <- "D:/Internship/data/compensated dataset/"

#parsing filenames and setting up patient dictionary
original_ds <- list.files(path=fcs_dir, pattern= '\\.fcs$', full.names=FALSE)
patient_ids <- as.integer(sub(".*[^0-9](\\d+)\\s+tube.*", "\\1", original_ds))
patient_file_dict <- split(original_ds, patient_ids)

#select which patients you want to extract: AML, MDS, healthy or fulldataset

#aml_patients = c(144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 196, 197, 198, 199, 201)

mds_patients = c(103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 186, 187, 188, 189, 192, 195, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211)

#healthy_patients = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50)

#full_dataset = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50,103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 186, 187, 188, 189, 192, 195, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 196, 197, 198, 199, 201)

if (exists("aml_patients")) {
  selected_patients <- aml_patients
} else if (exists("mds_patients")) {
  selected_patients <- mds_patients
} else if (exists("healthy_patients")) {
  selected_patients <- healthy_patients
} else if (exists("full_dataset")) {
  selected_patients <- full_dataset
} else {
  stop("No patient group was defined.")
}

fcs_file_names <- unlist(patient_file_dict[as.character(selected_patients)])
fcs_paths <- file.path(fcs_dir, fcs_file_names)

fcs_compensated <- setNames(lapply(fcs_paths, read.FCS, transformation = FALSE), fcs_file_names)
```

```{r}
#BEFORE QC DATASET SIZES (CAN BE SEEN AS RAW DATA)
raw_data_event_counts <- sapply(fcs_compensated, nrow)

raw_data_event_counts_df <- data.frame(
  Sample = names(raw_data_event_counts),
  Events = raw_data_event_counts
)
```

```{r}
GbLimit <- 2    

# Calculate size of each file (in GB) using paths
size_fcs <- file.size(fcs_paths) / 1024^3

# Compute cumulative size and number of groups
cums <- cumsum(size_fcs)
groups <- ceiling(sum(size_fcs) / GbLimit)

# Assign each file to a batch based on cumulative size
batches <- cut(cums, breaks = groups, labels = FALSE)

output_dir <- "D:/Internship/data/cleaned dataset"
dir.create(output_dir, showWarnings = FALSE)

```

During quality control it was seen that an error occured for some files, this was later seen that it was due to files having event recordings with timestap = 0. These may indicate several things: these events may be from before the acquisition formally started, or instrument reset or failed to stamp time correctly or it might be due to an interrupted run. Because of these facts it was chosen to remove all these files before performing QC

```{r}
for (i in seq_len(groups)) {
  cat("\nProcessing batch", i, "\n")
  
  # Indices of files in this batch
  batch_indices <- which(batches == i)

  for (j in batch_indices) {
    ff <- fcs_compensated[[j]]
    file_name <- basename(fcs_paths[j])
    proceed_flag <- TRUE

    # Run flow_auto_qc with error handling
    cleaned_ff <- tryCatch({
      flow_auto_qc(ff)
    }, error = function(e) {
      cat("Error in flow_auto_qc for", file_name, ":", e$message, "\n")
      proceed_flag <<- FALSE
      NULL
    })

    # Save cleaned file if no error
    if (proceed_flag && !is.null(cleaned_ff)) {
      output_fcs_path <- file.path(output_dir, file_name)
      write.FCS(cleaned_ff, filename = output_fcs_path)
      cat("Saved cleaned FCS:", output_fcs_path, "\n")
    }
  }

  # Optional: free memory
  gc()
}
```

```{r}

```

```{r}
#AUTOMATIC QUALITY CONTROL

#choose prefered batch_size for processing
batch_size <- 5

total_files <- length(fcs_compensated)
file_names <- names(fcs_compensated)
output_dir <- "D:/Internship/data/cleaned dataset"
dir.create(output_dir, showWarnings = FALSE)

#start QC in batches, while saving .fcs files per batch and resetting memory
for (batch_start in seq(1, total_files, by = batch_size)) {
  batch_end <- min(batch_start + batch_size - 1, total_files)
  batch_indices <- batch_start:batch_end
  cat("\nProcessing batch:", batch_start, "to", batch_end, "\n")

  for (i in batch_indices) {
    file_name <- file_names[i]
    ff <- fcs_compensated[[i]]
    proceed_flag <- TRUE

    tryCatch({
      cleaned_ff <- flow_auto_qc(ff)
    }, error = function(e) {
      cat("Error in flow_auto_qc for", file_name, ":", as.character(e$message), "\n")
      proceed_flag <- FALSE
    })

    if (proceed_flag) {
      output_fcs_path <- file.path(output_dir, paste0(file_name))
      write.FCS(cleaned_ff, filename = output_fcs_path)
      cat("Saved cleaned FCS:", output_fcs_path, "\n")
    }
  }

  # Optional: Free memory
  gc()
}
```

### STEP 1.4 - TRANSFORMATION

Compensation corrects for spectral overlap between fluorochromes but does not address the underlying distribution of flow cytometry data. Such data typically exhibits a positively skewed distribution, especially in fluorescent channels. To make the data more suitable for analysis and visualization, transformation methodsâ€”such as logarithmic, biexponential (logicle), or arcsinhâ€”are applied to achieve a more symmetric, near-normal distribution.

In this workflow, an **arcsinh transformation** is applied to the data, followed by **minâ€“max normalization**, in alignment with the methodology described by Duetz et al. (2021).

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
```

```{r}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

fcs_dir <- "D:/Internship/data/cleaned dataset/"

#parsing filenames and setting up patient dictionary
original_ds <- list.files(path=fcs_dir, pattern= '\\.fcs$', full.names=FALSE)
patient_ids <- as.integer(sub(".*[^0-9](\\d+)\\s+tube.*", "\\1", original_ds))
patient_file_dict <- split(original_ds, patient_ids)

#select which patients you want to extract: AML, MDS, healthy or fulldataset

#aml_patients = c(144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 196, 197, 198, 199, 201)

mds_patients = c(103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 186, 187, 188, 189, 192, 195, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211)

#healthy_patients = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50)

#full_dataset = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50,103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 186, 187, 188, 189, 192, 195, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 196, 197, 198, 199, 201)

if (exists("aml_patients")) {
  selected_patients <- aml_patients
} else if (exists("mds_patients")) {
  selected_patients <- mds_patients
} else if (exists("healthy_patients")) {
  selected_patients <- healthy_patients
} else if (exists("full_dataset")) {
  selected_patients <- full_dataset
} else {
  stop("No patient group was defined.")
}

fcs_file_names <- unlist(patient_file_dict[as.character(selected_patients)])
fcs_paths <- file.path(fcs_dir, fcs_file_names)

fcs_cleaned <- setNames(lapply(fcs_paths, read.FCS, transformation = FALSE), fcs_file_names)
```

```{r}
#BEFORE QC DATASET SIZES (CAN BE SEEN AS RAW DATA)
QC_data_event_counts <- sapply(fcs_cleaned, nrow)

QC_data_event_counts_df <- data.frame(
  Sample = names(QC_data_event_counts),
  Events = QC_data_event_counts
)
```

```{r}
#ARCSINH TRANSFORMATION
arcsinh_transform <- function(ff, cofactor = 150, exclude_channels = c("FS-A", "FS-H", "SS-A", "SS-H", "TIME")) {
  transform_channels <- setdiff(colnames(exprs(ff)), exclude_channels)
  
  arcsinh_tf <- transformList(
    transform_channels,
    arcsinhTransform(a = 0, b = 1 / cofactor, c = 0)
  )

  ff_transformed <- transform(ff, arcsinh_tf)
  
  return(ff_transformed)
}

fcs_transformed <- lapply(fcs_cleaned, arcsinh_transform, cofactor = 150)
```

```{r}
#CHECKING TRANSFORMED DATA
ff_trans <- fcs_transformed[[2]]
autoplot(ff_trans, x = "KO", y = "SS-A", bins = 200)
```

```{r}
#MIN-MAX NORMALIZATION
min_max_quantile_normalize <- function(ff, exclude_channels = c("FS-A", "FS-H", "SS-A", "SS-H", "TIME")) {
  expr <- exprs(ff)
  
  norm_channels <- setdiff(colnames(expr), exclude_channels)
  
  expr[, norm_channels] <- apply(expr[, norm_channels, drop = FALSE], 2, function(x) {
    (x - quantile(x, 0.001, na.rm = TRUE)) / 
      (quantile(x, 0.999, na.rm = TRUE) - quantile(x, 0.001, na.rm = TRUE))
  })
  
  flowCore::exprs(ff) <- expr
  return(ff)
}

fcs_norm <- vector("list", length(fcs_transformed))
names(fcs_norm) <- names(fcs_transformed)

for (i in seq_along(fcs_transformed)) {
  cat("Normalizing flowFrame", i, "of", length(fcs_transformed), "\n")
  fcs_norm[[i]] <- min_max_quantile_normalize(fcs_transformed[[i]])
}
```

```{r}
#CHECKING TRANSFORMED DATA
ff_norm <- fcs_norm[[2]]
autoplot(ff_norm, x = "KO", y = "SS-A", bins = 200)
```

### STEP 1.5: SAVING STANDARDIZED AND TRANSFORMED DATA FILES on DRIVE

```{r}
output_dir <- normalizePath("D:/Internship/data/transformed full dataset", mustWork = FALSE)
dir.create(output_dir, showWarnings = FALSE)

for (file_name in names(fcs_norm)) {
  file_path <- file.path(output_dir, paste0(file_name, ".fcs"))
  write.FCS(fcs_norm[[file_name]], filename = file_path)
}
```

## STEP 2: REMOVAL OF UNWANTED EVENTS

### STEP 2.1 - loading in libraries and files we want to clean up

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
library(dplyr)
```

```{r}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

fcs_dir <- "D:/Internship/data/transformed full dataset/"

#parsing filenames and setting up patient dictionary
original_ds <- list.files(path=fcs_dir, pattern= '\\.fcs$', full.names=FALSE)
patient_ids <- as.integer(sub(".*[^0-9](\\d+)\\s+tube.*", "\\1", original_ds))
patient_file_dict <- split(original_ds, patient_ids)

#select which patients you want to extract: AML, MDS, healthy or fulldataset

#aml_patients = c(144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 196, 197, 198, 199, 201)

mds_patients = c(103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 186, 187, 188, 189, 192, 195, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211)

#healthy_patients = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50)

#full_dataset = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50,103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 186, 187, 188, 189, 192, 195, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 196, 197, 198, 199, 201)

if (exists("aml_patients")) {
  selected_patients <- aml_patients
} else if (exists("mds_patients")) {
  selected_patients <- mds_patients
} else if (exists("healthy_patients")) {
  selected_patients <- healthy_patients
} else if (exists("full_dataset")) {
  selected_patients <- full_dataset
} else {
  stop("No patient group was defined.")
}

fcs_file_names <- unlist(patient_file_dict[as.character(selected_patients)])
fcs_paths <- file.path(fcs_dir, fcs_file_names)

fcs_files <- setNames(lapply(fcs_paths, read.FCS, transformation = FALSE), fcs_file_names)
```

```{r}
#BEFORE GATING DATASET SIZES (CAN BE SEEN AS TRANSFORMED DATA)
raw_data_event_counts <- sapply(fcs_files, nrow)
raw_data_event_counts_df <- data.frame(
  Sample = names(raw_data_event_counts),
  Events = raw_data_event_counts
)
```

### STEP 2.2 - Gating out scatter outliers

In order to gate out debris, the function `flowClust.2d` is employed. This performs automated gating by applying model-based clustering FS-A and SS-A channels. It uses the `flowClust` algorithm, which fits a mixture of multivariate t-distributions to the selected channels, modeling the data as a combination of distinct cell populations (clusters).

In this case 2 clusters are chosen, one for debris and the other for cells. `flowClust.2d` then identifies the main cluster of interest (usually the largest or most biologically relevant cluster) based on membership probabilities. It converts this cluster's boundary into a polygon gate using the `clust2Poly` function, which defines the spatial region of this population in the 2D parameter space.

This approach provides a statistically rigorous, unsupervised way to separate cell populations from debris or outliers based on scatter properties, allowing for consistent and reproducible gating prior to downstream analysis.

```{r}
ff_raw <- fcs_files[[2]]
df_raw <- as.data.frame(exprs(ff_raw))
```

```{r}
my_gates <- lapply(seq_along(fcs_files), function(i) {
  cat("Processing sample", i, "of", length(fcs_files), "\n")
  
  ff <- fcs_files[[i]]
  
  # Extract the two channels for clustering
  ch_data <- exprs(ff)[, c("FS-A", "SS-A")]
  
  # Check for valid, non-empty, non-NA data
  if (nrow(ch_data) == 0 || all(is.na(ch_data))) {
    warning(paste("Sample", i, "is empty after gating. Skipping."))
    return(NULL)
  }

  if (nrow(na.omit(ch_data)) < 10) {
    warning(paste("Sample", i, "has too few events after gating. Skipping."))
    return(NULL)
  }
  
  # Call clustering only if data is valid
  openCyto:::.flowClust.2d(ff, channels = c("FS-A", "SS-A"), K = 2)
})
```

```{r}
fcs_gated <- mapply(Subset, fcs_files, my_gates, SIMPLIFY = FALSE)
```

```{r}
#CHECKING GATED DATA
ff_gated <- fcs_gated[[2]]
df_gated <- as.data.frame(exprs(ff_gated))
```

```{r}
#AFTER GATING DATASET SIZES
gated_data_event_counts <- sapply(fcs_gated, nrow)
gated_data_event_counts_df <- data.frame(
  Sample = names(gated_data_event_counts),
  Events = gated_data_event_counts
)

```

### STEP 2.3 - Removing doublets

We can use a developed R code section from the Duetz et al. (2021) paper to remove doublets from the data

```{r}
is_single <- function(ff, plot = FALSE, ...) {
  fsc_a <- flowCore::exprs(ff)[,"FS-A"]
  fsc_h <- flowCore::exprs(ff)[,"FS-H"]
  
  bins <- cut(fsc_a, 10)
  
  ratios <- fsc_h / fsc_a
  slope_per_bin <- tapply(ratios, bins, mean)
  expected_values <- fsc_a * slope_per_bin[bins]
  deviations <- abs(fsc_h - expected_values)
  
  x <- tapply(fsc_a, bins, mean)
  e <- tapply(expected_values, bins, mean)
  d <- tapply(deviations, bins, function(x){mean(x) + 2*sd(x)})
  y <- e - d
  
  spl <- splinefun(x, y)
  
  if (plot) {
    flowDensity::plotDens(ff, c("FS-A", "FS-H"), ...)
    points(x, e, col = "red", pch = 19)
    points(x, y, col = "red", pch = 19)
    lines(seq(1, 300000, by = 1000), 
          spl(seq(1, 300000, by = 1000)),
          col = "red",
          lwd = 2)
  }

  selection <- fsc_h > spl(fsc_a)
  return(selection)
}

single_cells_list <- vector("list", length(fcs_gated))
names(single_cells_list) <- names(fcs_gated)

for (i in seq_along(fcs_gated)) {
  cat(sprintf("Processing sample %d of %d: %s\n", i, length(fcs_gated), names(fcs_gated)[i]))
  single_cells_list[[i]] <- is_single(fcs_gated[[i]], plot = FALSE)
}

fcs_singles <- mapply(function(ff, sel) {
  ff[sel, ]  # keep rows where sel == TRUE
}, fcs_gated, single_cells_list, SIMPLIFY = FALSE)
```

```{r}
#INSPECTING DOUBLET REMOVED DATA
ff_single <- fcs_singles[[2]]
autoplot(ff_single, x = "KO", y = "SS-A", bins = 200)
```

```{r}
#AFTER DOUBLET REMOVAL DATASET SIZES
single_data_event_counts <- sapply(fcs_singles, nrow)
single_data_event_counts_df <- data.frame(
  Sample = names(single_data_event_counts),
  Events = single_data_event_counts
)
```

### STEP 2.4 - Saving preprocessed files

```{r}
library(flowCore)

#output folder
out_dir <- "D:/Internship/data/preprocessed dataset"
dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

for (name in names(fcs_singles)) {
  ff <- fcs_singles[[name]]
  filename <- file.path(out_dir, paste0(tools::file_path_sans_ext(name)))
  write.FCS(ff, filename)
  cat("Saved:", filename, "\n")
}
```

### STEP 2.5 - VISUALISATION of preprocessing pipeline

```{r}
library(flowCore)
library(ggplot2)
library(dplyr)

#which .fcs file do you want to visualize?
raw_ff <- read.FCS("D:/Internship/data/transformed full dataset/Ki67-145 tube 2.fcs", transformation = FALSE)
preprocessed_ff <- read.FCS("D:/Internship/data/preprocessed dataset/Ki67-145 tube 2_prepro.fcs", transformation = FALSE)

#which columns do you want to visualize?
col_to_visualize <- c("FS-H", "FS-A", "SS-A", "KO")

raw_expr <- exprs(raw_ff)
pre_expr <- exprs(preprocessed_ff)


rounded_raw <- as.data.frame(round(raw_expr[, col_to_visualize], 3))
rounded_pre <- as.data.frame(round(pre_expr[, col_to_visualize], 3))

rounded_raw$Status <- "Removed"

kept <- semi_join(rounded_raw, rounded_pre, by = c("FS-H", "FS-A"))

kept$Status <- "Kept"

removed <- anti_join(rounded_raw, kept, by = c("FS-H", "FS-A"))

plot_df <- rbind(kept, removed)

#Adapt x, y, axis names, and title to plot you want to make
ggplot(plot_df, aes(x = `KO`, y = `SS-A`, color = Status)) +
  geom_point(size = 0.5, alpha = 0.6) +
  scale_color_manual(values = c("Kept" = "blue", "Removed" = "red")) +
  theme_minimal() +
  labs(title = "After preprocessing: KO vs. SSC-A", x = "CD45", y = "SSC-A")
```

```{r}
autoplot(preprocessed_ff, x = "KO", y = "SS-A", bins = 200)
```

## STEP 3: EXTRACTING EVENT COUNTS FOR EDA

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
library(dplyr)
```

```{r}
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, 
                      message = FALSE, cache.lazy = FALSE)

fcs_dir <- "D:/Internship/data/preprocessed dataset/"

#parsing filenames and setting up patient dictionary
original_ds <- list.files(path=fcs_dir, pattern= '\\.fcs$', full.names=FALSE)
patient_ids <- as.integer(sub(".*[^0-9](\\d+)\\s+tube.*", "\\1", original_ds))
patient_file_dict <- split(original_ds, patient_ids)

#select which patients you want to extract: AML, MDS, healthy or fulldataset

#aml_patients = c(144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 196, 197, 198, 199, 201)

mds_patients = c(103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 186, 187, 188, 189, 192, 195, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211)

#healthy_patients = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50)

#full_dataset = c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50,103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 186, 187, 188, 189, 192, 195, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 196, 197, 198, 199, 201)

if (exists("aml_patients")) {
  selected_patients <- aml_patients
} else if (exists("mds_patients")) {
  selected_patients <- mds_patients
} else if (exists("healthy_patients")) {
  selected_patients <- healthy_patients
} else if (exists("full_dataset")) {
  selected_patients <- full_dataset
} else {
  stop("No patient group was defined.")
}

fcs_file_names <- unlist(patient_file_dict[as.character(selected_patients)])
fcs_paths <- file.path(fcs_dir, fcs_file_names)

fcs_files <- setNames(lapply(fcs_paths, read.FCS, transformation = FALSE), fcs_file_names)
```

```{r}
#SAVING EVENT COUNT AS EXCEL
event_count <- sapply(fcs_files, nrow)
event_count_df <- data.frame(
  Sample = names(event_count),
  Events = event_count
)

#change filename to what you selected
write.csv(event_count_df, "D:/Internship/data/mds_patients_prepro.csv", row.names = FALSE)

```

## STEP 4: AGGREGATE FILES

We aggregate files per tube for all patients within one disease group. This way we end up with 7 .fcs files representing the 7 tubes measured for that specific disease state. This aggregated file is established by randomly selecting 40,000 cells across all patient .fcs files to ensure representative sampling. This aggregation step was necessary, as the FlowSOM algorithm requires a single .fcs file as input for training and clustering.

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
library(dplyr)
```

```{r}
tubes <- paste0("tube ", 1:7)
fcs_dir <- "D:/Internship/data/preprocessed dataset/healthy/"
output_dir <- "D:/Internship/data/results/"
dir.create(output_dir, showWarnings = FALSE)

aggregate_cells <- 40000
set.seed(2020)

all_fcs_files <- list.files(fcs_dir, pattern = "\\.fcs$", full.names = FALSE)
```

```{r}
for (tube in tubes) {
  cat("ðŸ”„ Processing", tube, "...\n")
  
  tube_file_names <- all_fcs_files[grepl(tube, all_fcs_files, ignore.case = TRUE)]
  
  if (length(tube_file_names) == 0) {
    warning(paste("âš ï¸ No files found for", tube, "- skipping."))
    next
  }
  
  tube_file_paths <- file.path(fcs_dir, tube_file_names)
  
  #perform aggregation per tube
  agg <- FlowSOM::AggregateFlowFrames(
    fileNames = tube_file_paths,
    cTotal = aggregate_cells,
    writeOutput = TRUE,
    outputFile = file.path(output_dir, paste0(tube, "_aggregate.fcs"))
  )
  
  cat("âœ… Aggregated", length(tube_file_paths), "files for", tube, "\n")
}
```

```{r}
#RENAMING COLUMNS PER TUBE TO WHAT THEY REPRESENT
ff <- read.FCS("D:/Internship/data/results/tube 7_aggregate.fcs")
colnames(ff)
```

These are the names of the columns per tube:

-   **Tube 1:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "IgG1-1", "NA1", "IgG1-2", "CD13", "CD117", "CD34", "NA2", "NA3", "HLA-DR", "CD45",Â "TIME", "File", "File_scattered", "Original_ID")

-   **Tube 2:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "Ki-67", "CD14", "CD-64", "CD-13", "CD117", "CD34", "CD10", "CD11b", "HLA-DR", "CD45",Â "TIME", "File", "File_scattered", "Original_ID")

-   **Tube 3:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "Ki-67", "CD105", "CD123", "CD33", "CD117", "NA1", "CD71", "CD235a", "HLA-DR", "CD45",Â "TIME", "File", "File_scattered", "Original_ID")

-   **Tube 4:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A","Ki-67", "NA1","NA2","CD33", "CD117", "NA3", "NA4", "NA5","HLA-DR", "CD45", "TIME", "File", "File_scattered", "Original_ID")

-   **Tube 5:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "CD16", "CD64", "Bcl-2", "CD13", "CD117", "CD34", "CD10", "CD14", "HLA-DR", "CD45",Â "TIME", "File", "File_scattered", "Original_ID")

-   **Tube 6:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "NA1", "CD105", "Bcl-2", "CD33", "CD117", "NA2", "CD71", "CD235a", "HLA-DR", "CD45",Â "TIME", "File", "File_scattered", "Original_ID")

-   **Tube 7:** c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "NA1", "NA2", "Bcl-2", "CD33", "CD117", "CD36", "NA3","NA4", "HLA-DR", "CD45", "TIME", "File", "File_scattered", "Original_ID")

```{r}
colnames(ff) <- c("FSC-H", "FSC-A", "SSC-H", "SSC-A", "NA1", "NA2", "Bcl-2", "CD33", "CD117", "CD36", "NA3","NA4", "HLA-DR", "CD45", "TIME", "File", "File_scattered", "Original_ID")

write.FCS(ff, filename = "D:/Internship/data/results/tube 7_final.fcs")
```

## STEP 5: APPLYING FLOWSOM

```{r}
library('flowCore') 
library('ggplot2')
library('flowAI')
library('readxl')
library('ggcyto')
library('openCyto')
library('flowClust')
library(dplyr)
```

```{r}
flow_frame <- read.FCS("D:/Internship/data/results/tube 2_final.fcs")
colnames(flow_frame)
```

SSC-A is important for gating, so we will use it as a parameter in the FlowSOM algorithm. However, before we do so, we need to transform this data in order to align with the other channels.

```{r}
#Define reference marker
reference_marker <- "CD45"

expr <- exprs(flow_frame)
if (!(reference_marker %in% colnames(expr)) || !("SSC-A" %in% colnames(expr))) {
  stop("Reference marker or SSC-A not found in the flowFrame.")
}

q5_goal   <- quantile(expr[, reference_marker], 0.05, na.rm = TRUE)
q95_goal  <- quantile(expr[, reference_marker], 0.95, na.rm = TRUE)
q5_SSCA   <- quantile(expr[, "SSC-A"], 0.05, na.rm = TRUE)
q95_SSCA  <- quantile(expr[, "SSC-A"], 0.95, na.rm = TRUE)
SSCA_a <- (q95_goal - q5_goal) / (q95_SSCA - q5_SSCA)
SSCA_b <- q5_goal - SSCA_a * q5_SSCA

#linear transformation
lin_trans <- transformList("SSC-A", linearTransform(a = SSCA_a, b = SSCA_b))
flow_frame <- transform(flow_frame, lin_trans)

#Min-Max normalization
min_max_ssca_only <- function(ff) {
  expr <- exprs(ff)
  
  if (!"SSC-A" %in% colnames(expr)) {
    stop("SSC-A channel not found in the flowFrame.")
  }
  
  # Compute quantiles for normalization
  q_min <- quantile(expr[, "SSC-A"], 0.001, na.rm = TRUE)
  q_max <- quantile(expr[, "SSC-A"], 0.999, na.rm = TRUE)
  
  # Apply min-max normalization
  expr[, "SSC-A"] <- (expr[, "SSC-A"] - q_min) / (q_max - q_min)
  
  # Cap values between 0 and 1 (optional, but recommended)
  expr[, "SSC-A"] <- pmin(pmax(expr[, "SSC-A"], 0), 1)
  
  # Update and return normalized flowFrame
  flowCore::exprs(ff) <- expr
  return(ff)
}

ff_transformed <- min_max_ssca_only(flow_frame)
```

```{r}
dir_results <- "D:/Internship/data/results/"
SOM_x <- 15 
SOM_y <- 15 
n_meta <- 10 
seed <- 2020 
scaling <- FALSE

df_flow_frame <- as.data.frame(exprs(ff_transformed))
```

```{r}
library(FlowSOM)
fsom <- FlowSOM(input = ff_transformed, scale = scaling, 
                colsToUse = c(4:14),
                seed = seed, 
                nClus = n_meta, 
                xdim = SOM_x, 
                ydim = SOM_y)
```

Plot the FlowSOM tree structure to visualize the clustering results. The `PlotStars` function generates a star plot, which is a common way to visualize FlowSOM results, showing the clusters and their relationships.

```{r}
p <- PlotStars(fsom = fsom, backgroundValues = fsom$metaclustering, maxNodeSize=2.5)
ggsave(filename = paste0(dir_results, "fsom_tree_tube2.pdf"), plot = p, height = 8.5, width = 11)
```

Create a report summarizing the FlowSOM results, including cluster information and visualizations. The `FlowSOMmary` function generates a comprehensive report that can be saved as a PDF file.

```{r}
FlowSOMmary(fsom, plotFile = "D:/Internship/data/results/fsom_tree_tube2_report.pdf")
```

### Inspecting cells in 2D scatterplots

before we interpret the FlowSOM clutering, we need to make sure clustering was done correctly. There does not exist a single comprehensive metric to assess the quality, however different visualizations and scores can help to verify the FlowSOM results.

```{r}
channel_pairs = list(c("CD34", "CD45"), c("CD45", "SSC-A"), c("CD14", "SSC-A"), c("CD14", "Ki-67"), c("HLA-DR", "CD117"), c("CD-13", "SSC-A")) 

metaclusters_of_interest <- seq_len(n_meta) 
clusters_of_interest <- NULL

Plot2DScatters(fsom = fsom, channelpairs = channel_pairs, metaclusters = metaclusters_of_interest, clusters = clusters_of_interest, plotFile = paste0(dir_results, "fsom_2D_scatters.png"), centers = FALSE, density=FALSE)
```

```         
```
