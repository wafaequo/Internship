library(nlme)
glsfit.1 <- gls(EWL ~ Period * ColorCode, data = primaryLME, correlation = corAR1(form = ~ 1 | ID))
glsfit.2 <- update(glsfit.1, correlation = corCompSymm(form = ~ 1 | ID))
lmefit.1 <- lme(fixed = EWL ~ Period*ColorCode, random = ~ 1 | ID,
data = primaryLME)
lmefit.2 <- update(lmefit.1, correlation = corAR1(form = ~1 | ID))
lmefit.3 <- update(lmefit.1, correlation = corCompSymm(form = ~ 1 | ID))
anova(glsfit.1, glsfit.2, lmefit.1, lmefit.2, lmefit.3)
summary(glsfit.1)
#filtering and ordering
FilteredData <- primary[, c("ID", "ColorCodeF", "Months")]
FilteredData <- FilteredData[order(FilteredData$ID, -FilteredData$Months), ]
#removing all months smaller than highest found for ID
duplicated_ids <- duplicated(FilteredData$ID)
followup <- FilteredData[!duplicated_ids, ]
#renaming months column to LastVisit.
colnames(followup)[3] <- "LastVisit"
followup$DropOut <- followup$LastVisit < 42
followup$ColorCodeF <- ifelse(followup$ColorCodeF %in% c("Orange", "Red"), "OrangeRed", "Green")
library(survival)
library(ggsurvfit)
#Kaplan-Meier Curve
KM_curve <- survfit(formula = Surv(followup$LastVisit, followup$DropOut) ~ followup$ColorCodeF, data = followup)
KM_plot <- ggsurvfit(KM_curve) + theme_classic() + scale_color_manual(name = "Color Code",
labels = c("Green", "OrangeRed"),
values = c("forestgreen", "darkorange"))+ ggtitle("Kaplan_Meier Survival Curves devided by Color Code") + xlab("Time (Months)")
KM_plot
#log-rank test
LR_test <- survdiff(formula = Surv(followup$LastVisit, followup$DropOut) ~ followup$ColorCodeF, data = followup)
LR_test
#Cox Proportional Hazards model
CoxPH <- coxph(formula = Surv(followup$LastVisit, followup$DropOut) ~ followup$ColorCodeF, data = followup)
summary(CoxPH)
#PREPROCESSING OF DATA
library(dplyr)
PreOp <- primary[primary$PeriodF == 'Pre-Op', ] #all PreOp measurements
PreOp$'nonHDL Cholesterol' <- PreOp$Cholesterol - PreOp$'HDL Cholesterol'
PreOp <- PreOp %>%
select(-"Date", -"Period", -"Months", -"ColorCode", -"ScreeningDate", -"ProcedureDate", -"Height", -"MedHeight", -"Weight", -"PreOpWeightSD", -"EWL", -"OrderID", -"PeriodF", -'Cholesterol', -'HDL Cholesterol', -'ID') #filtering out all columns that are deemed irrelevant for predicting EWLSuccess
#for all missing BMIs replace it with mean of dataset as missingness is deemed MCAR
PreOp$BMI[is.na(PreOp$BMI)]<- mean(PreOp$BMI,na.rm = TRUE)
#remove all NA's
PreOp <- na.omit(PreOp)
#convert target y variable to numeric
PreOp$EWLSuccess <- as.integer(PreOp$EWLSuccess)
#FINDING CONFOUDING VARIABLES AND REMOVING THEM
library(corrplot)
corr_data <- PreOp[, c(2,4:35, 37)]
#Compute correlation matrix
cor_matrix <- cor(corr_data, use = "complete.obs")
high_cor <- which(abs(cor_matrix) >= 0.6 & abs(cor_matrix) != 1, arr.ind = TRUE) #gives the indices of the elements that meet condition.
#visualizing cm
high_cor_df <- data.frame(
Var1 = rownames(cor_matrix)[high_cor[, 1]],
Var2 = colnames(cor_matrix)[high_cor[, 2]],
Correlation = cor_matrix[high_cor]
)
print(high_cor_df)
#remove one of the two variables with high correlation
PreOp <- PreOp %>%
select(-"Glucose", -"ALAT") #removing both glucose and ALAT.
#log-10 scale for continuous variables
names_nolog10_var <- c('Procedure', 'EWLSuccess', 'BMI', 'ColorCodeF')
all_columns <- colnames(PreOp)
names_log10_var <- setdiff(all_columns, names_nolog10_var)
input_df <- data.frame(matrix(ncol = 0, nrow = nrow(PreOp)))
for (var in names_log10_var) {
input_df[[var]] <- log10(PreOp[[var]])
}
for (var in names_nolog10_var) {
input_df[[var]] <- PreOp[[var]]
}
#this results in 596 sample size with 34 variable to train with.
#DATA ANALYSIS
library(rsample)
set.seed(421) #for reproducability (number is from datacamp tutorial)
split <- initial_split(input_df, prop = 0.8, strata = EWLSuccess) #stratified sampling
train <- split %>%
training()
test <- split %>%
testing()
cat("Training set dimensions:", dim(train), "\n")
cat("Testing set dimensions:", dim(test), "\n")
X_train <- model.matrix(EWLSuccess ~ .- 1, data = train)
Y_train <- train$EWLSuccess
X_test <- model.matrix(EWLSuccess ~ .- 1, data = test)
Y_test <- test$EWLSuccess
#CHECKING THRESHOLD VALUE
#TRAINING THE MODEL
library(glmnet)
#fitting the model using cross validation and lasso regression
cvfit <- cv.glmnet(X_train, Y_train, family = "binomial", type.measure = "auc", alpha = 1)
#to get better view of fit
#print(cvfit)
plot(cvfit$glmnet.fit,"lambda", label=TRUE) #trace plot
plot(cvfit) #lambda.min = λ with minimum mean cross-validation error. lambda.1se = λ with most regularized model such that cross-validated error is within one standard error of the minimum.
#standardized coefficients
coef_min <- coef(cvfit, s = cvfit$lambda.min)
print(coef_min)
#TRAINING THE MODEL
library(glmnet)
#fitting the model using cross validation and lasso regression
cvfit <- cv.glmnet(X_train, Y_train, family = "binomial", type.measure = "auc", alpha = 1)
#to get better view of fit
print(cvfit)
plot(cvfit$glmnet.fit,"lambda", label=TRUE) #trace plot
plot(cvfit) #lambda.min = λ with minimum mean cross-validation error. lambda.1se = λ with most regularized model such that cross-validated error is within one standard error of the minimum.
#standardized coefficients
coef_min <- coef(cvfit, s = cvfit$lambda.min)
print(coef_min)
#TRAINING THE MODEL
library(glmnet)
#fitting the model using cross validation and lasso regression
cvfit <- cv.glmnet(X_train, Y_train, family = "binomial", type.measure = "auc", alpha = 1)
#to get better view of fit
print(cvfit)
plot(cvfit$glmnet.fit,"lambda", label=TRUE) #trace plot
plot(cvfit) #lambda.min = λ with minimum mean cross-validation error. lambda.1se = λ with most regularized model such that cross-validated error is within one standard error of the minimum.
#standardized coefficients
coef_min <- coef(cvfit, s = cvfit$lambda.min)
library(pROC)
#Function that completes confusion matrix when it misses a column due to emptyness.
checking_cm <- function(cm) {
classes <- c("0", "1")
if (!all(classes %in% colnames(cm))) {
missing_cols <- setdiff(classes, colnames(cm))
for (col in missing_cols) {
cm <- cbind(cm, matrix(0, nrow = nrow(cm), ncol = 1, dimnames = list(Actual = rownames(cm), Predicted = col)))
}
cm <- cm[, sort(colnames(cm))] #Making sure all cm have same order of columns, so positional indices of TP,TN,FP,FN make sense.
}
return(cm)
}
pred_probs <- predict(cvfit, newx = X_test, s = cvfit$lambda.min, type = "response")
true_probs <- Y_test
#Assessing predictive ability using AUC
rocEWLsuccess <- roc(Y_test, pred_probs)
aucEWLsuccess <- auc(rocEWLsuccess)
print(paste("AUC:", round(aucEWLsuccess, 2)))
#assessing sens, spec, PPV, NPV using Youden Index as cutoff
predictive_coords <- coords(rocEWLsuccess, "best",best.method = "youden", ret = c("thres", "sens", "spec"))
thres <- predictive_coords[1]
sens <- predictive_coords[2]
spec <- predictive_coords[3]
pred_class <- ifelse(pred_probs > as.numeric(thres), 1, 0) #classifying predictions based on optimal threshold
cm <- table(Actual = Y_test, Predicted = pred_class)
cm <- checking_cm(cm)
TN <- cm[1, 1]
FN <- cm[2, 1]
FP <- cm[1, 2]
TP <- cm[2, 2]
sensitivity <- TP / (TP+FN)
specificity <- TN / (TN+FP)
PPV <- TP / (TP + FP)       #precision and dependent on prevalence
NPV <- TN / (TN + FN)
#PERFORMANCE METRICS
metrics <- list(
Optimal_Threshold = round(thres, 2),
Sensitivity = round(sens, 2),
Specificity = round(spec, 2),
PPV = round(PPV, 2),
NPV = round(NPV, 2),
AUC = round(aucEWLsuccess, 2)
)
print(metrics)
library(pROC)
#Function that completes confusion matrix when it misses a column due to emptyness.
checking_cm <- function(cm) {
classes <- c("0", "1")
if (!all(classes %in% colnames(cm))) {
missing_cols <- setdiff(classes, colnames(cm))
for (col in missing_cols) {
cm <- cbind(cm, matrix(0, nrow = nrow(cm), ncol = 1, dimnames = list(Actual = rownames(cm), Predicted = col)))
}
cm <- cm[, sort(colnames(cm))] #Making sure all cm have same order of columns, so positional indices of TP,TN,FP,FN make sense.
}
return(cm)
}
pred_probs <- predict(cvfit, newx = X_test, s = cvfit$lambda.min, type = "response")
true_probs <- Y_test
#Assessing predictive ability using AUC
rocEWLsuccess <- roc(Y_test, pred_probs)
aucEWLsuccess <- auc(rocEWLsuccess)
print(paste("AUC:", round(aucEWLsuccess, 2)))
#assessing sens, spec, PPV, NPV using Youden Index as cutoff
predictive_coords <- coords(rocEWLsuccess, "best",best.method = "youden", ret = c("thres", "sens", "spec"))
thres <- predictive_coords[1]
sens <- predictive_coords[2]
spec <- predictive_coords[3]
pred_class <- ifelse(pred_probs > as.numeric(thres), 1, 0) #classifying predictions based on optimal threshold
cm <- table(Actual = Y_test, Predicted = pred_class)
cm <- checking_cm(cm)
TN <- cm[1, 1]
FN <- cm[2, 1]
FP <- cm[1, 2]
TP <- cm[2, 2]
sensitivity <- TP / (TP+FN)
specificity <- TN / (TN+FP)
PPV <- TP / (TP + FP)       #precision and dependent on prevalence
NPV <- TN / (TN + FN)
#PERFORMANCE METRICS
metrics <- list(
Optimal_Threshold = round(thres, 2),
Sensitivity = round(sens, 2),
Specificity = round(spec, 2),
PPV = round(PPV, 2),
NPV = round(NPV, 2),
AUC = round(aucEWLsuccess, 2)
)
print(metrics)
coefficients = cvfit.coef_[0]
library(pROC)
#Function that completes confusion matrix when it misses a column due to emptyness.
checking_cm <- function(cm) {
classes <- c("0", "1")
if (!all(classes %in% colnames(cm))) {
missing_cols <- setdiff(classes, colnames(cm))
for (col in missing_cols) {
cm <- cbind(cm, matrix(0, nrow = nrow(cm), ncol = 1, dimnames = list(Actual = rownames(cm), Predicted = col)))
}
cm <- cm[, sort(colnames(cm))] #Making sure all cm have same order of columns, so positional indices of TP,TN,FP,FN make sense.
}
return(cm)
}
pred_probs <- predict(cvfit, newx = X_test, s = cvfit$lambda.min, type = "response")
true_probs <- Y_test
#Assessing predictive ability using AUC
rocEWLsuccess <- roc(Y_test, pred_probs)
aucEWLsuccess <- auc(rocEWLsuccess)
print(paste("AUC:", round(aucEWLsuccess, 2)))
#assessing sens, spec, PPV, NPV using Youden Index as cutoff
predictive_coords <- coords(rocEWLsuccess, "best",best.method = "youden", ret = c("thres", "sens", "spec"))
thres <- predictive_coords[1]
sens <- predictive_coords[2]
spec <- predictive_coords[3]
pred_class <- ifelse(pred_probs > as.numeric(thres), 1, 0) #classifying predictions based on optimal threshold
cm <- table(Actual = Y_test, Predicted = pred_class)
cm <- checking_cm(cm)
TN <- cm[1, 1]
FN <- cm[2, 1]
FP <- cm[1, 2]
TP <- cm[2, 2]
sensitivity <- TP / (TP+FN)
specificity <- TN / (TN+FP)
PPV <- TP / (TP + FP)       #precision and dependent on prevalence
NPV <- TN / (TN + FN)
#PERFORMANCE METRICS
metrics <- list(
Optimal_Threshold = round(thres, 2),
Sensitivity = round(sens, 2),
Specificity = round(spec, 2),
PPV = round(PPV, 2),
NPV = round(NPV, 2),
AUC = round(aucEWLsuccess, 2)
)
print(metrics)
coefficients <- coef(cvfit, s = "lambda.min")
print(coefficients)
getRversion()
surgery <- read.csv("surgeries.csv")
lab <- read.csv("lab results.csv")
hweight <- read.csv("heights weights.csv")
preop <- read.csv("preop.csv")
str(surgery)
surgery$ID <- factor(surgery$ID)
surgery$Procedure <- factor(surgery$Procedure)
surgery$ProcedureDate <- as.Date(surgery$ProcedureDate)
str(surgery)
ProcedureCount <- table(surgery$Procedure)
ProcedureFrequencies <- sort(ProcedureCount, decreasing = TRUE)
ProcedureFrequenciesDf <- as.data.frame(ProcedureFrequencies)
print(ProcedureFrequenciesDf)
year <- as.numeric(format(surgery$ProcedureDate, "%Y"))
crosstable <- table(surgery$Procedure, year)
crosstable
surgeryCount <- table(surgery$ID)
surgeryFrequency <- table(surgeryCount)
surgeryFrequency
IDpatients <- names(surgeryCount[surgeryCount == 3])
threeSurgeriesPatients <- subset(surgery, surgery$ID %in% IDpatients)
threeSurgeriesPatients
surgery <- surgery[order(surgery$ID, surgery$ProcedureDate), ]
surgery$PrimarySurgery <- !(duplicated(surgery$ID))
hweight$ID <- factor(hweight$ID)
hweight$Variable <- factor(hweight$Variable)
hweight$Unit <- factor(hweight$Unit)
hweight$Date <- as.Date(hweight$Date)
str(hweight)
lab$ID = factor(lab$ID)
lab$TestName = factor(lab$TestName)
lab$Unit = factor(lab$Unit)
lab$Date = as.Date(lab$Date)
str(lab)
NApercentage <- function(x) {
numConversion <- as.numeric(x)
percentageNA <- sum(is.na(numConversion)) / length(numConversion) * 100
return(percentageNA)
}
NApercentages <- tapply(lab$Result, lab$TestName, NApercentage)
NAdataframe <- subset(data.frame(
TestName = names(NApercentages),
NApercentage = NApercentages
), NApercentage >= 5)
NAdataframe <- NAdataframe[order(NAdataframe$NApercentage, decreasing = TRUE), ]
rownames(NAdataframe) <- NULL
print(NAdataframe)
lab$ResultNum <- as.numeric(gsub('[><]', '', lab$Result))
preop$Date <- as.Date(preop$Date)
preop$ID <- factor(preop$ID)
preop$ColorCode <- as.ordered(preop$ColorCode)
str(preop)
library(data.table)
hweight <- as.data.table(hweight)
lab <- as.data.table(lab)
preop <- as.data.table(preop)
surgery <- as.data.table(surgery)
labWide <- dcast(lab, ID+Date+OrderID~TestName, value.var='ResultNum')
head(labWide)
labWide[, NResults := rowSums(!is.na(.SD)), .SDcols = 4:(ncol(labWide)-1)]
setorder(labWide, -NResults)
freqtable <- table(labWide$NResults)
freqtable
barplot(freqtable, xlab = "# of tests", ylab = "frequency")
hweight[, Dupl := .N >= 2, by = .(ID, Date, Variable)]
table(hweight$Dupl)
hweight[Dupl == TRUE, Diff := abs(Result - min(Result)), by = .(ID, Variable)]
hweightFilt <- hweight[Diff <= 10 | is.na(Diff)]
rowsdropped <- nrow(hweight) - nrow(hweightFilt)
rowsdropped
hweightWide <- dcast(hweight, ID+Date ~ Variable, value.var = "Result", fun.aggregate = mean)
head(hweightWide)
hweightWide[, MedHeight := median(Height, na.rm=TRUE), by = ID]
formulaBMI <- function(weight, height) {
bmi <- weight / ((height / 100)^2)
return(round(bmi, 1))
}
hweightWide[, BMI := formulaBMI(Weight, MedHeight), by = ID]
#IDs in surgery with no match in hWeight
nomatchHweight <- surgery[!ID %in% hweightWide$ID, ID]
#IDs in surgery with no match in Lab
nomatchLab <- surgery[!ID %in% labWide$ID, ID]
nomatchsurgery <- intersect(nomatchHweight, nomatchLab)
nomatchsurgery
library(lubridate)
setorder(surgery, ID, ProcedureDate)
ProcedureDateMin <- surgery[, .(ProcedureDateMin = min(ProcedureDate)), by = ID]
surgery <- merge(surgery, ProcedureDateMin, by = "ID", all.x = TRUE)
surgery[, PrimaryProcedureMin6M := ProcedureDateMin %m+% months(-6)]
surgery[, ProcedureDateMin := NULL]
hweightWide[,date_copy := Date]
surgeryHweightMerged <- hweightWide[surgery, on = .(ID, date_copy >= PrimaryProcedureMin6M)]
surgeryHweightMerged[, date_copy := NULL]
dim(surgeryHweightMerged)
labWide[,date_copy := Date]
surgeryLabMerged <- labWide[surgery, on=.(ID, date_copy>=PrimaryProcedureMin6M)]
surgeryLabMerged[, date_copy := NULL]
dim(surgeryLabMerged)
combinedData <- rbindlist(list(surgeryHweightMerged, surgeryLabMerged), fill=TRUE)
setorder(combinedData, ID, Date)
dim(combinedData)
library(ggplot2)
library(lubridate)
setorder(combinedData, ID)
MinProcedureDate <- combinedData[, .(MinProcedureDate = min(ProcedureDate)), by = ID]
combinedData <- merge(combinedData, MinProcedureDate, by = "ID", all.x = TRUE)
combinedData[, Months :=round((difftime(Date, MinProcedureDate, units = "days")/30.44),2)]
combinedData[, Months := gsub(" days", "", Months)] #remove the words days from column MinProcedureDate
combinedData[, MinProcedureDate := NULL]
summary(surgeryMerged$Months)
library(lubridate)
setorder(combinedData, ID)
MinProcedureDate <- combinedData[, .(MinProcedureDate = min(ProcedureDate)), by = ID]
combinedData <- merge(combinedData, MinProcedureDate, by = "ID", all.x = TRUE)
combinedData[, Months :=round((difftime(Date, MinProcedureDate, units = "days")/30.44),2)]
combinedData[, Months := gsub(" days", "", Months)] #remove the words days from column MinProcedureDate
combinedData[, MinProcedureDate := NULL]
summary(combinedData$Months)
combinedData[, Months := as.numeric(as.character(Months))]
MonthsHistogram <- ggplot(combinedData, aes(x = Months)) +
geom_histogram(binwidth = 1, fill='#735DA5', color='#D3C5E5') +
labs(title = "Distribution of Months before/after primary surgery",
x = "Months",
y = "Frequency") +
theme_grey()
MonthsHistogram
preOpData <- combinedData[Date <= ProcedureDate]
preOpBMI <- preOpData[, .(PreOpBMI = round(mean(BMI, na.rm = TRUE), 1)), by = ID]
#merge
combinedData <- merge(combinedData, preOpBMI, by = "ID", all.x = TRUE)
preOpBMIData <- unique(combinedData[!is.na(PreOpBMI)], by = "ID")
PreOpBMIHistogram <- ggplot(preOpBMIData, aes(x = PreOpBMI)) +
geom_histogram(binwidth = 2, fill='#2C5F2D', color='#A7BEAE') +
labs(title = "Distribution of Pre-operative BMI",
x = "Pre-operative BMI",
y = "Frequency") +
theme_grey()
print(PreOpBMIHistogram)
labels <- c("Pre-Op", "3M", "6M", "12M", "24M", "36M", "48M", "60M or more")
breakpoints <- c(-Inf, 0, 4, 9, 18, 30, 42, 54, Inf)
combinedData[, Period := cut(Months, breakpoints, labels, ordered_result = TRUE)]
data.table(table(combinedData$Period))
meanBMI <- combinedData[, .(MeanBMI = round(mean(BMI, na.rm=TRUE),1)), by = .(ID, Period)]
ggplot(meanBMI, aes(x = MeanBMI, fill = Period)) +
geom_histogram(binwidth = 2, position = "identity", alpha = 0.8) +
scale_colour_viridis_d() +
labs(title = "BMI Distribution Per Period",
x = "Mean BMI",
y = "Frequency") +
theme_grey()
BMIpanel <- ggplot(meanBMI, aes(x = MeanBMI, fill = Period)) +
geom_histogram(binwidth = 2, position = "identity", alpha = 0.8) +
scale_colour_viridis_d() +
labs(title = "BMI Distribution Per Period",
x = "Mean BMI",
y = "Frequency") +
theme_minimal() +
facet_wrap(~ Period, ncol = 1)
print(BMIpanel)
ggplot(combinedData, aes(x = Months, y = BMI, color=ID, alpha=0.8)) +
geom_line(show.legend = FALSE)+
labs(title = "Spaghetti Plot of BMI over Time",
x = "Months",
y = "BMI") +
theme_grey()
combinedData[, PreOpWeight := round(mean(Weight[Period == 'Pre-Op'], na.rm=TRUE),2), by=ID]
combinedData[, PostOpWeight := ifelse(Period != 'Pre-Op', Weight, NA), by = ID]
combinedData[, Ideal := 25*((MedHeight/100)^2)]
calculateEWL <- function(PreOp, PostOp, Height) {
IdealWeight <- 25 * ((Height / 100)^2)
EWL <- (((PreOp - PostOp) / (PreOp - IdealWeight)) * 100)
return(round(EWL, 1))
}
combinedData[, EWL := calculateEWL(PreOpWeight, PostOpWeight, MedHeight), by=.(ID)]
combinedData[, PreOpWeight := round(mean(Weight[Period == 'Pre-Op'], na.rm=TRUE),2), by=ID]
combinedData[, PostOpWeight := ifelse(Period != 'Pre-Op', Weight, NA), by = ID]
combinedData[, Ideal := 25*((MedHeight/100)^2)]
calculateEWL <- function(PreOp, PostOp, Height) {
IdealWeight <- 25 * ((Height / 100)^2)
EWL <- (((PreOp - PostOp) / (PreOp - IdealWeight)) * 100)
return(round(EWL, 1))
}
combinedData[, EWL := calculateEWL(PreOpWeight, PostOpWeight, MedHeight), by=.(ID)]
PostOpData <- combinedData[!(Period == 'Pre-Op') & PrimarySurgery == TRUE & !(is.na(EWL))]
# Create the histogram plot
EWLhistogram <- ggplot(PostOpData, aes(x = EWL)) +
geom_histogram(binwidth = 10, fill = "#2B061E", color = "#875053") +
labs(title = "EWL% for Post-Op",
x = "Excess Weight Loss (%)",
y = "Frequency") +
theme_grey()
print(EWLhistogram)
scatterData <- combinedData[PreOpBMI >= 35 & !(Period == 'Pre-Op') & PrimarySurgery == TRUE & PreOpBMI >= 35 & EWL <= 200 & EWL >= -50]
EWLscatterplot <-  ggplot(scatterData, aes(x = Months, y = EWL, color = Procedure)) +
geom_point(alpha=0.2) +
geom_smooth(method = "loess", se = FALSE) +
scale_color_manual(values = c("Sleeve" = "#F9C80E", "Bypass" = "#43BCCD", "Sleeve>Bypass" = '#F86624', 'RevisionBypass' = '#EA3546', 'Sleeve>SADI' = '#662E9B', 'RemovalAGB'='#31263E', 'Sleeve>BPD' = '#731963', 'AGB>Bypass' = '#0B3C49')) +
labs(title = "Scatterplot EWL% Bypass vs. Sleeve",
x = "Months", y = "EWL (%)") +
theme_grey()
print(EWLscatterplot)
TwoYMeasurements <- combinedData[Months > 0 & Months <=24]
TwoYMeasurements <- TwoYMeasurements[, .(EWLSuccess = tail(na.omit(EWL), 1) >= 50), by = ID]
# Merge the result back to the original data by 'ID'
combinedData <- merge(combinedData, TwoYMeasurements, by = "ID", all.x = TRUE)
EWLTcount <- unique(combinedData[EWLSuccess == TRUE], by = "ID")
EWLFcount <- unique(combinedData[EWLSuccess == FALSE], by = "ID")
EWLNAcount <- unique(combinedData[is.na(EWLSuccess)], by = "ID")
EWLcountTable <- data.frame(
EWL = c("TRUE", "FALSE", "NA"),
Count = c(length(EWLTcount$ID), length(EWLFcount$ID), length(EWLNAcount$ID))
)
print(EWLcountTable)
combinedData[, nonHDL := Cholesterol - `HDL Cholesterol`]
#HbA1c to check for diabetes. Reference range 20-42
HbA1cPlot <- ggplot(combinedData, aes(x = Months, y = HbA1c, color=ID, alpha=0.8)) +
geom_line(show.legend = FALSE) +
labs(x = "Months", y = "HbA1c", title = "Spaghetti plot of HbA1c")
#non-HDL for risk for heart disease (lipid panel). Ref: <3.4 mmol/L
nonHDLPlot <- ggplot(combinedData, aes(x = Months, y = nonHDL, color=ID, alpha=0.8)) +
geom_line(show.legend= FALSE) +
labs(x = "Months", y = "non-HDL", title = "Spaghetti plot of non-HDL")
print(HbA1cPlot)
print(nonHDLPlot)
PreOpBPanels <- combinedData[NResults >= 28 & Period == 'Pre-Op']
#select lab data
start_index <- which(names(PreOpBPanels) == "ALAT")
end_index <- which(names(PreOpBPanels) == "Vitamin D")
PreOpLabResults <- PreOpBPanels[, start_index:end_index]
corrmatrix <- round(cor(PreOpLabResults, method='pearson', use='pairwise.complete.obs'), digit=4)
library(corrplot)
corrplot(corrmatrix, method='color', type='upper')
corrtable <- data.table(corrmatrix, keep.rownames = TRUE)
corrmelt <- melt(corrtable, id.vars = "rn", measure.vars = colnames(corrtable)[2:ncol(corrtable)])
highcorr <- corrmelt[abs(value) >= 0.5 & value != 1]
print(highcorr)
pcaPreOpLab <- prcomp(corrmatrix, scale. = TRUE)
variance <- pcaPreOpLab$sdev^2 / sum(pcaPreOpLab$sdev^2)
screeData <- data.frame(PC = 1:length(variance), Variance = variance)
library(ggplot2)
ggplot(screeData, aes(x = PC, y = Variance)) +
geom_bar(stat = "identity", fill = "#E2B6CF", width = 0.5) +
geom_line(color = "#E396DF") +
labs(x = "Principal Component", y = "Variance Explained", title = "Scree Plot") +
theme_grey()
library(factoextra)
fviz_pca_var(pcaPreOpLab, select.var = list(cos2 = 0.3), gradient.cols=c("red", "blue"), col.var='cos2')
# Manually set Bioconductor repositories
options(BioC_mirror = "https://bioconductor.org")
options(repos = BiocManager::repositories())
options(timeout = 300)  # 5-minute timeout
BiocManager::repositories()
BiocManager::install("flowCore", verbose = TRUE)
